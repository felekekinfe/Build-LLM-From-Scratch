{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3535, -0.1955],\n",
      "        [ 0.1574, -0.2035],\n",
      "        [ 0.1563, -0.1833],\n",
      "        [ 0.1585, -0.1544],\n",
      "        [ 0.0919,  0.2323],\n",
      "        [ 0.1709, -0.3557]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1576, -0.2091, -0.2039, -0.1210, -0.0537, -0.1782],\n",
      "        [ 0.0801,  0.1690,  0.1650,  0.1048,  0.0470,  0.1488],\n",
      "        [ 0.0789,  0.1629,  0.1591,  0.1008,  0.0452,  0.1433],\n",
      "        [ 0.0789,  0.1570,  0.1533,  0.0967,  0.0433,  0.1378],\n",
      "        [ 0.0347,  0.0069,  0.0066, -0.0004, -0.0004,  0.0029],\n",
      "        [ 0.0916,  0.2187,  0.2136,  0.1374,  0.0617,  0.1938]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1661, 0.1602, 0.1608, 0.1705, 0.1788, 0.1637],\n",
      "        [0.1621, 0.1726, 0.1721, 0.1649, 0.1583, 0.1701],\n",
      "        [0.1624, 0.1723, 0.1719, 0.1649, 0.1586, 0.1700],\n",
      "        [0.1628, 0.1721, 0.1716, 0.1649, 0.1588, 0.1698],\n",
      "        [0.1698, 0.1665, 0.1665, 0.1656, 0.1656, 0.1660],\n",
      "        [0.1595, 0.1745, 0.1738, 0.1647, 0.1561, 0.1714]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mask where the value above the diagonal is zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1661, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1621, 0.1726, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1624, 0.1723, 0.1719, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1628, 0.1721, 0.1716, 0.1649, 0.0000, 0.0000],\n",
      "        [0.1698, 0.1665, 0.1665, 0.1656, 0.1656, 0.0000],\n",
      "        [0.1595, 0.1745, 0.1738, 0.1647, 0.1561, 0.1714]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1661],\n",
      "        [0.3346],\n",
      "        [0.5066],\n",
      "        [0.6715],\n",
      "        [0.8340],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4843, 0.5157, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3206, 0.3402, 0.3393, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2425, 0.2563, 0.2556, 0.2456, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1996, 0.1996, 0.1986, 0.1986, 0.0000],\n",
      "        [0.1595, 0.1745, 0.1738, 0.1647, 0.1561, 0.1714]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4688, -0.3531],\n",
      "        [-0.5099, -0.1380],\n",
      "        [-0.5192, -0.0738],\n",
      "        [-0.4657, -0.0238],\n",
      "        [-0.4096, -0.0585],\n",
      "        [-0.4163, -0.0007]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1576, -0.2091, -0.2039, -0.1210, -0.0537, -0.1782],\n",
      "        [ 0.0801,  0.1690,  0.1650,  0.1048,  0.0470,  0.1488],\n",
      "        [ 0.0789,  0.1629,  0.1591,  0.1008,  0.0452,  0.1433],\n",
      "        [ 0.0789,  0.1570,  0.1533,  0.0967,  0.0433,  0.1378],\n",
      "        [ 0.0347,  0.0069,  0.0066, -0.0004, -0.0004,  0.0029],\n",
      "        [ 0.0916,  0.2187,  0.2136,  0.1374,  0.0617,  0.1938]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0.1576,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0801,  0.1690,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0789,  0.1629,  0.1591,    -inf,    -inf,    -inf],\n",
      "        [ 0.0789,  0.1570,  0.1533,  0.0967,    -inf,    -inf],\n",
      "        [ 0.0347,  0.0069,  0.0066, -0.0004, -0.0004,    -inf],\n",
      "        [ 0.0916,  0.2187,  0.2136,  0.1374,  0.0617,  0.1938]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1576, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0801,  0.1690,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0789,  0.1629,  0.1591,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0789,  0.1570,  0.1533,  0.0967,  0.0000,  0.0000],\n",
      "        [ 0.0347,  0.0069,  0.0066, -0.0004, -0.0004,  0.0000],\n",
      "        [ 0.0916,  0.2187,  0.2136,  0.1374,  0.0617,  0.1938]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score*mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4889, 0.5111, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3243, 0.3382, 0.3375, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2447, 0.2544, 0.2540, 0.2469, 0.0000, 0.0000],\n",
      "        [0.2025, 0.1997, 0.1997, 0.1990, 0.1990, 0.0000],\n",
      "        [0.1616, 0.1722, 0.1717, 0.1653, 0.1592, 0.1700]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.3204, 0.0000, 0.0000, 0.3575, 0.3274],\n",
      "        [0.0000, 0.3451, 0.0000, 0.3298, 0.0000, 0.3402],\n",
      "        [0.3248, 0.3447, 0.3437, 0.0000, 0.3171, 0.3399],\n",
      "        [0.3257, 0.0000, 0.0000, 0.0000, 0.3176, 0.0000],\n",
      "        [0.3396, 0.3330, 0.3329, 0.0000, 0.3312, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3477, 0.0000, 0.0000, 0.3428]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        attn_score=queries@keys.T\n",
    "        attn_weight=torch.softmax(attn_score/keys[1]**0.5,dim=1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "y=torch.zeros(3,3)\n",
    "for r in range(3):\n",
    "    row=x[r]\n",
    "    row[r+1:y.append(row)]=row[r+1:]*0\n",
    "    y[r][r]\n",
    "    \n",
    "print(y)\n",
    "\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
