{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3270, -0.5156],\n",
      "        [-0.0844, -0.7772],\n",
      "        [-0.0986, -0.7681],\n",
      "        [ 0.0397, -0.4323],\n",
      "        [-0.3273, -0.3878],\n",
      "        [ 0.1701, -0.5485]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7913e-01,  1.3354e-01,  1.3731e-01,  4.4606e-02,  1.6665e-01,\n",
      "          1.4401e-02],\n",
      "        [ 3.1231e-01,  7.8952e-02,  7.8960e-02, -2.0569e-04,  5.7149e-02,\n",
      "          5.0050e-03],\n",
      "        [ 3.0707e-01,  8.2581e-02,  8.2800e-02,  2.3079e-03,  6.3705e-02,\n",
      "          5.5682e-03],\n",
      "        [ 1.8268e-01,  1.7962e-02,  1.6763e-02, -1.4421e-02, -9.3755e-03,\n",
      "         -7.5910e-04],\n",
      "        [ 1.2628e-01,  1.2482e-01,  1.2879e-01,  4.6993e-02,  1.6402e-01,\n",
      "          1.4160e-02],\n",
      "        [ 2.4420e-01, -1.3052e-02, -1.6234e-02, -3.8059e-02, -6.8750e-02,\n",
      "         -5.8567e-03]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1745, 0.1690, 0.1694, 0.1587, 0.1730, 0.1553],\n",
      "        [0.1947, 0.1651, 0.1651, 0.1561, 0.1625, 0.1566],\n",
      "        [0.1937, 0.1653, 0.1653, 0.1561, 0.1631, 0.1565],\n",
      "        [0.1852, 0.1648, 0.1647, 0.1611, 0.1617, 0.1626],\n",
      "        [0.1696, 0.1694, 0.1699, 0.1603, 0.1742, 0.1567],\n",
      "        [0.1952, 0.1627, 0.1623, 0.1599, 0.1564, 0.1635]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mask where the value above the diagonal is zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1947, 0.1651, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1937, 0.1653, 0.1653, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1852, 0.1648, 0.1647, 0.1611, 0.0000, 0.0000],\n",
      "        [0.1696, 0.1694, 0.1699, 0.1603, 0.1742, 0.0000],\n",
      "        [0.1952, 0.1627, 0.1623, 0.1599, 0.1564, 0.1635]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1745],\n",
      "        [0.3597],\n",
      "        [0.5243],\n",
      "        [0.6757],\n",
      "        [0.8433],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5412, 0.4588, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3695, 0.3152, 0.3153, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2740, 0.2439, 0.2437, 0.2384, 0.0000, 0.0000],\n",
      "        [0.2011, 0.2009, 0.2014, 0.1901, 0.2065, 0.0000],\n",
      "        [0.1952, 0.1627, 0.1623, 0.1599, 0.1564, 0.1635]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1012e-04, -1.6020e-01],\n",
      "        [-7.1134e-02, -2.2580e-01],\n",
      "        [-1.0036e-01, -2.4875e-01],\n",
      "        [-9.5312e-02, -2.3328e-01],\n",
      "        [-1.3272e-01, -2.1261e-01],\n",
      "        [-1.0512e-01, -2.1596e-01]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7913e-01,  1.3354e-01,  1.3731e-01,  4.4606e-02,  1.6665e-01,\n",
      "          1.4401e-02],\n",
      "        [ 3.1231e-01,  7.8952e-02,  7.8960e-02, -2.0569e-04,  5.7149e-02,\n",
      "          5.0050e-03],\n",
      "        [ 3.0707e-01,  8.2581e-02,  8.2800e-02,  2.3079e-03,  6.3705e-02,\n",
      "          5.5682e-03],\n",
      "        [ 1.8268e-01,  1.7962e-02,  1.6763e-02, -1.4421e-02, -9.3755e-03,\n",
      "         -7.5910e-04],\n",
      "        [ 1.2628e-01,  1.2482e-01,  1.2879e-01,  4.6993e-02,  1.6402e-01,\n",
      "          1.4160e-02],\n",
      "        [ 2.4420e-01, -1.3052e-02, -1.6234e-02, -3.8059e-02, -6.8750e-02,\n",
      "         -5.8567e-03]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.1791,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.3123,  0.0790,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.3071,  0.0826,  0.0828,    -inf,    -inf,    -inf],\n",
      "        [ 0.1827,  0.0180,  0.0168, -0.0144,    -inf,    -inf],\n",
      "        [ 0.1263,  0.1248,  0.1288,  0.0470,  0.1640,    -inf],\n",
      "        [ 0.2442, -0.0131, -0.0162, -0.0381, -0.0688, -0.0059]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1791,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3123,  0.0790,  0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3071,  0.0826,  0.0828,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1827,  0.0180,  0.0168, -0.0144, -0.0000, -0.0000],\n",
      "        [ 0.1263,  0.1248,  0.1288,  0.0470,  0.1640,  0.0000],\n",
      "        [ 0.2442, -0.0131, -0.0162, -0.0381, -0.0688, -0.0059]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score*mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5291, 0.4709, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3587, 0.3206, 0.3207, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2668, 0.2457, 0.2456, 0.2418, 0.0000, 0.0000],\n",
      "        [0.2008, 0.2006, 0.2010, 0.1930, 0.2046, 0.0000],\n",
      "        [0.1865, 0.1640, 0.1637, 0.1619, 0.1594, 0.1645]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3491, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3301, 0.0000, 0.3251, 0.3133],\n",
      "        [0.0000, 0.3305, 0.3306, 0.3123, 0.3262, 0.3130],\n",
      "        [0.0000, 0.0000, 0.3293, 0.3222, 0.0000, 0.3253],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3483, 0.3133],\n",
      "        [0.3903, 0.3254, 0.0000, 0.3197, 0.3128, 0.3271]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal attention Class with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.transpose(1,2))#change row 1 with colmn 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        \n",
    "        b,num_token,d_in=input_embedding.shape\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        print(keys.shape[-1])\n",
    "\n",
    "        attn_score=queries@keys.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
    "        \n",
    "        print('attn_score',attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/keys[-1]**0.5,dim=-1)\n",
    "        print('attn_weight',attn_weight)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        masked_context_vectore=attn_weight@values\n",
    "        return masked_context_vectore\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "in_d,out_d=3,2\n",
    "print(in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Causal_Attention(\n",
      "  (w_q): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (w_k): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (w_v): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2\n",
      "attn_score tensor([[[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]],\n",
      "\n",
      "        [[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m ca\u001b[38;5;241m=\u001b[39mCausal_Attention(in_d,out_d,context_length,\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ca)\n\u001b[0;32m----> 6\u001b[0m context_vec\u001b[38;5;241m=\u001b[39m\u001b[43mca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 25\u001b[0m, in \u001b[0;36mCausal_Attention.forward\u001b[0;34m(self, input_embedding)\u001b[0m\n\u001b[1;32m     22\u001b[0m attn_score\u001b[38;5;241m.\u001b[39mmasked_fill_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mbool()[:num_token, :num_token],\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_score\u001b[39m\u001b[38;5;124m'\u001b[39m,attn_score)\n\u001b[0;32m---> 25\u001b[0m attn_weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mattn_score\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_weight\u001b[39m\u001b[38;5;124m'\u001b[39m,attn_weight)\n\u001b[1;32m     27\u001b[0m attn_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weight)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "ca=Causal_Attention(in_d,out_d,context_length,0.0)\n",
    "print(ca)\n",
    "context_vec=ca.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "y=torch.zeros(3,3)\n",
    "for r in range(3):\n",
    "    row=x[r]\n",
    "    row[r+1:y.append(row)]=row[r+1:]*0\n",
    "    y[r][r]\n",
    "    \n",
    "print(y)\n",
    "\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
