{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0557,  0.3296],\n",
      "        [ 0.3878,  0.5322],\n",
      "        [ 0.3798,  0.5047],\n",
      "        [ 0.2754,  0.3604],\n",
      "        [ 0.1288, -0.1322],\n",
      "        [ 0.3635,  0.6427]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1139, -0.1506, -0.1426, -0.0993,  0.0417, -0.1816],\n",
      "        [-0.0469, -0.2759, -0.2640, -0.1998,  0.0241, -0.3223],\n",
      "        [-0.0411, -0.2625, -0.2512, -0.1905,  0.0218, -0.3064],\n",
      "        [-0.0281, -0.1877, -0.1797, -0.1363,  0.0151, -0.2190],\n",
      "        [ 0.0762,  0.0531,  0.0497,  0.0311, -0.0264,  0.0664],\n",
      "        [-0.0868, -0.3260, -0.3114, -0.2326,  0.0386, -0.3828]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1657, 0.1615, 0.1624, 0.1674, 0.1850, 0.1580],\n",
      "        [0.1825, 0.1552, 0.1565, 0.1638, 0.1919, 0.1502],\n",
      "        [0.1821, 0.1557, 0.1570, 0.1639, 0.1904, 0.1510],\n",
      "        [0.1778, 0.1589, 0.1598, 0.1647, 0.1834, 0.1554],\n",
      "        [0.1707, 0.1680, 0.1676, 0.1654, 0.1588, 0.1696],\n",
      "        [0.1817, 0.1534, 0.1550, 0.1639, 0.1985, 0.1474]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mask where the value above the diagonal is zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1657, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1825, 0.1552, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1821, 0.1557, 0.1570, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1778, 0.1589, 0.1598, 0.1647, 0.0000, 0.0000],\n",
      "        [0.1707, 0.1680, 0.1676, 0.1654, 0.1588, 0.0000],\n",
      "        [0.1817, 0.1534, 0.1550, 0.1639, 0.1985, 0.1474]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1657],\n",
      "        [0.3377],\n",
      "        [0.4948],\n",
      "        [0.6612],\n",
      "        [0.8304],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5404, 0.4596, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3681, 0.3147, 0.3172, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2690, 0.2403, 0.2416, 0.2491, 0.0000, 0.0000],\n",
      "        [0.2056, 0.2023, 0.2018, 0.1991, 0.1912, 0.0000],\n",
      "        [0.1817, 0.1534, 0.1550, 0.1639, 0.1985, 0.1474]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1667, -0.2088],\n",
      "        [ 0.2543, -0.4122],\n",
      "        [ 0.2803, -0.4893],\n",
      "        [ 0.2765, -0.4624],\n",
      "        [ 0.1975, -0.4792],\n",
      "        [ 0.2208, -0.4564]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1139, -0.1506, -0.1426, -0.0993,  0.0417, -0.1816],\n",
      "        [-0.0469, -0.2759, -0.2640, -0.1998,  0.0241, -0.3223],\n",
      "        [-0.0411, -0.2625, -0.2512, -0.1905,  0.0218, -0.3064],\n",
      "        [-0.0281, -0.1877, -0.1797, -0.1363,  0.0151, -0.2190],\n",
      "        [ 0.0762,  0.0531,  0.0497,  0.0311, -0.0264,  0.0664],\n",
      "        [-0.0868, -0.3260, -0.3114, -0.2326,  0.0386, -0.3828]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0.1139,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0469, -0.2759,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0411, -0.2625, -0.2512,    -inf,    -inf,    -inf],\n",
      "        [-0.0281, -0.1877, -0.1797, -0.1363,    -inf,    -inf],\n",
      "        [ 0.0762,  0.0531,  0.0497,  0.0311, -0.0264,    -inf],\n",
      "        [-0.0868, -0.3260, -0.3114, -0.2326,  0.0386, -0.3828]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1139, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0469, -0.2759, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0411, -0.2625, -0.2512, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0281, -0.1877, -0.1797, -0.1363,  0.0000, -0.0000],\n",
      "        [ 0.0762,  0.0531,  0.0497,  0.0311, -0.0264,  0.0000],\n",
      "        [-0.0868, -0.3260, -0.3114, -0.2326,  0.0386, -0.3828]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score*mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5286, 0.4714, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3577, 0.3202, 0.3220, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2633, 0.2431, 0.2441, 0.2495, 0.0000, 0.0000],\n",
      "        [0.2040, 0.2016, 0.2013, 0.1994, 0.1938, 0.0000],\n",
      "        [0.1774, 0.1574, 0.1585, 0.1649, 0.1888, 0.1530]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3130, 0.0000, 0.3837, 0.3004],\n",
      "        [0.0000, 0.3114, 0.3139, 0.3277, 0.3808, 0.3019],\n",
      "        [0.0000, 0.0000, 0.3195, 0.3295, 0.0000, 0.3108],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.3391],\n",
      "        [0.3634, 0.3069, 0.0000, 0.3278, 0.3971, 0.2948]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal attention Class with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.transpose(1,2))#change row(1) with colmn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        \n",
    "        b,num_token,d_in=input_embedding.shape\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        print(keys.shape[-1])\n",
    "\n",
    "        attn_score=queries@keys.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
    "        \n",
    "        print('attn_score',attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/keys[-1]**0.5,dim=-1)\n",
    "        print('attn_weight',attn_weight)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        masked_context_vectore=attn_weight@values\n",
    "        return masked_context_vectore\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "in_d,out_d=3,2\n",
    "print(in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Causal_Attention(\n",
      "  (w_q): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (w_k): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (w_v): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2\n",
      "attn_score tensor([[[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]],\n",
      "\n",
      "        [[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m ca\u001b[38;5;241m=\u001b[39mCausal_Attention(in_d,out_d,context_length,\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ca)\n\u001b[0;32m----> 6\u001b[0m context_vec\u001b[38;5;241m=\u001b[39m\u001b[43mca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 25\u001b[0m, in \u001b[0;36mCausal_Attention.forward\u001b[0;34m(self, input_embedding)\u001b[0m\n\u001b[1;32m     22\u001b[0m attn_score\u001b[38;5;241m.\u001b[39mmasked_fill_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mbool()[:num_token, :num_token],\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_score\u001b[39m\u001b[38;5;124m'\u001b[39m,attn_score)\n\u001b[0;32m---> 25\u001b[0m attn_weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mattn_score\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_weight\u001b[39m\u001b[38;5;124m'\u001b[39m,attn_weight)\n\u001b[1;32m     27\u001b[0m attn_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weight)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "ca=Causal_Attention(in_d,out_d,context_length,0.0)\n",
    "print(ca)\n",
    "context_vec=ca.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "y=torch.zeros(3,3)\n",
    "for r in range(3):\n",
    "    row=x[r]\n",
    "    row[r+1:y.append(row)]=row[r+1:]*0\n",
    "    y[r][r]\n",
    "    \n",
    "print(y)\n",
    "\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
