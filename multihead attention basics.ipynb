{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1654,  0.2226],\n",
      "        [-0.0417,  0.5664],\n",
      "        [-0.0454,  0.5493],\n",
      "        [ 0.0107,  0.3694],\n",
      "        [-0.0993,  0.0858],\n",
      "        [ 0.0432,  0.5531]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0588, -0.1890, -0.1850, -0.1207, -0.0614, -0.1651],\n",
      "        [-0.1896, -0.4089, -0.4014, -0.2481, -0.1534, -0.3344],\n",
      "        [-0.1833, -0.3974, -0.3901, -0.2414, -0.1488, -0.3254],\n",
      "        [-0.1276, -0.2595, -0.2549, -0.1559, -0.0997, -0.2095],\n",
      "        [-0.0189, -0.0796, -0.0778, -0.0520, -0.0239, -0.0717],\n",
      "        [-0.1940, -0.3834, -0.3766, -0.2293, -0.1491, -0.3075]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1751, 0.1597, 0.1602, 0.1676, 0.1748, 0.1625],\n",
      "        [0.1784, 0.1528, 0.1536, 0.1712, 0.1830, 0.1610],\n",
      "        [0.1782, 0.1531, 0.1539, 0.1710, 0.1826, 0.1612],\n",
      "        [0.1733, 0.1579, 0.1584, 0.1699, 0.1768, 0.1636],\n",
      "        [0.1708, 0.1637, 0.1639, 0.1669, 0.1702, 0.1646],\n",
      "        [0.1759, 0.1539, 0.1546, 0.1716, 0.1816, 0.1624]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1784, 0.1528, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1782, 0.1531, 0.1539, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1733, 0.1579, 0.1584, 0.1699, 0.0000, 0.0000],\n",
      "        [0.1708, 0.1637, 0.1639, 0.1669, 0.1702, 0.0000],\n",
      "        [0.1759, 0.1539, 0.1546, 0.1716, 0.1816, 0.1624]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1751],\n",
      "        [0.3312],\n",
      "        [0.4853],\n",
      "        [0.6596],\n",
      "        [0.8354],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5387, 0.4613, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3672, 0.3156, 0.3172, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2628, 0.2394, 0.2402, 0.2576, 0.0000, 0.0000],\n",
      "        [0.2045, 0.1959, 0.1961, 0.1997, 0.2038, 0.0000],\n",
      "        [0.1759, 0.1539, 0.1546, 0.1716, 0.1816, 0.1624]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2130, -0.6795],\n",
      "        [ 0.3920, -0.8111],\n",
      "        [ 0.4560, -0.8569],\n",
      "        [ 0.4368, -0.7743],\n",
      "        [ 0.4082, -0.7247],\n",
      "        [ 0.4111, -0.7052]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0588, -0.1890, -0.1850, -0.1207, -0.0614, -0.1651],\n",
      "        [-0.1896, -0.4089, -0.4014, -0.2481, -0.1534, -0.3344],\n",
      "        [-0.1833, -0.3974, -0.3901, -0.2414, -0.1488, -0.3254],\n",
      "        [-0.1276, -0.2595, -0.2549, -0.1559, -0.0997, -0.2095],\n",
      "        [-0.0189, -0.0796, -0.0778, -0.0520, -0.0239, -0.0717],\n",
      "        [-0.1940, -0.3834, -0.3766, -0.2293, -0.1491, -0.3075]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0.0588,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1896, -0.4089,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1833, -0.3974, -0.3901,    -inf,    -inf,    -inf],\n",
      "        [-0.1276, -0.2595, -0.2549, -0.1559,    -inf,    -inf],\n",
      "        [-0.0189, -0.0796, -0.0778, -0.0520, -0.0239,    -inf],\n",
      "        [-0.1940, -0.3834, -0.3766, -0.2293, -0.1491, -0.3075]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5274, 0.4726, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3571, 0.3209, 0.3220, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2590, 0.2425, 0.2431, 0.2554, 0.0000, 0.0000],\n",
      "        [0.2032, 0.1971, 0.1973, 0.1998, 0.2027, 0.0000],\n",
      "        [0.1732, 0.1576, 0.1581, 0.1702, 0.1772, 0.1637]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3503, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3072, 0.0000, 0.3661, 0.3221],\n",
      "        [0.0000, 0.3063, 0.3079, 0.3420, 0.3652, 0.3223],\n",
      "        [0.0000, 0.0000, 0.3169, 0.3398, 0.0000, 0.3272],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3404, 0.3291],\n",
      "        [0.3519, 0.3078, 0.0000, 0.3432, 0.3632, 0.3247]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal attention Class with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.transpose(1,2))#change row(1) with colmn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        \n",
    "        b,num_token,d_in=input_embedding.shape\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        print(keys.shape[-1])\n",
    "\n",
    "        attn_score=queries@keys.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
    "        \n",
    "        print('attn_score',attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "        print('attn_weight',attn_weight)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        masked_context_vectore=attn_weight@values\n",
    "        return masked_context_vectore\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "in_d,out_d=3,3\n",
    "print(in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Causal_Attention(\n",
      "  (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "3\n",
      "attn_score tensor([[[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]],\n",
      "\n",
      "        [[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0755,  0.2528,  0.4010],\n",
       "         [ 0.0065,  0.5118,  0.5036],\n",
       "         [ 0.0316,  0.5984,  0.5364],\n",
       "         [ 0.0758,  0.6148,  0.4682],\n",
       "         [ 0.0707,  0.6636,  0.4533],\n",
       "         [ 0.0984,  0.6533,  0.4289]],\n",
       "\n",
       "        [[-0.0755,  0.2528,  0.4010],\n",
       "         [ 0.0065,  0.5118,  0.5036],\n",
       "         [ 0.0316,  0.5984,  0.5364],\n",
       "         [ 0.0758,  0.6148,  0.4682],\n",
       "         [ 0.0707,  0.6636,  0.4533],\n",
       "         [ 0.0984,  0.6533,  0.4289]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "ca=Causal_Attention(in_d,out_d,context_length,0.0)\n",
    "print(ca)\n",
    "\n",
    "ca.forward(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi head attention wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent,num_head):\n",
    "\n",
    "        super().__init__()\n",
    "        self.heads=torch.nn.ModuleList([Causal_Attention(in_d,out_d,context_length,dropout_percent) for _ in range(num_head)])\n",
    "        print('self head',self.heads)\n",
    "    def forward(self,inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "self head ModuleList(\n",
      "  (0-1): 2 x Causal_Attention(\n",
      "    (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "3\n",
      "attn_score tensor([[[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]],\n",
      "\n",
      "        [[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3\n",
      "attn_score tensor([[[-0.7599,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1980, -1.5578,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1836, -1.5390, -1.5286,    -inf,    -inf,    -inf],\n",
      "         [-0.7215, -0.9420, -0.9343, -0.6817,    -inf,    -inf],\n",
      "         [-0.6178, -0.8005, -0.7966, -0.5784, -0.5878,    -inf],\n",
      "         [-0.8995, -1.1750, -1.1648, -0.8494, -0.7740, -0.9742]],\n",
      "\n",
      "        [[-0.7599,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1980, -1.5578,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1836, -1.5390, -1.5286,    -inf,    -inf,    -inf],\n",
      "         [-0.7215, -0.9420, -0.9343, -0.6817,    -inf,    -inf],\n",
      "         [-0.6178, -0.8005, -0.7966, -0.5784, -0.5878,    -inf],\n",
      "         [-0.8995, -1.1750, -1.1648, -0.8494, -0.7740, -0.9742]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3797, 0.3092, 0.3111, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2640, 0.2324, 0.2335, 0.2701, 0.0000, 0.0000],\n",
      "         [0.2065, 0.1858, 0.1863, 0.2113, 0.2101, 0.0000],\n",
      "         [0.1732, 0.1477, 0.1486, 0.1783, 0.1862, 0.1659]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3797, 0.3092, 0.3111, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2640, 0.2324, 0.2335, 0.2701, 0.0000, 0.0000],\n",
      "         [0.2065, 0.1858, 0.1863, 0.2113, 0.2101, 0.0000],\n",
      "         [0.1732, 0.1477, 0.1486, 0.1783, 0.1862, 0.1659]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_veccccccccc tensor([[[-0.0755,  0.2528,  0.4010, -0.1045,  0.3258, -0.2155],\n",
      "         [ 0.0065,  0.5118,  0.5036,  0.1106,  0.2283, -0.1851],\n",
      "         [ 0.0316,  0.5984,  0.5364,  0.1942,  0.1897, -0.1705],\n",
      "         [ 0.0758,  0.6148,  0.4682,  0.2526,  0.2154, -0.0963],\n",
      "         [ 0.0707,  0.6636,  0.4533,  0.2856,  0.2013, -0.0392],\n",
      "         [ 0.0984,  0.6533,  0.4289,  0.2972,  0.2215, -0.0311]],\n",
      "\n",
      "        [[-0.0755,  0.2528,  0.4010, -0.1045,  0.3258, -0.2155],\n",
      "         [ 0.0065,  0.5118,  0.5036,  0.1106,  0.2283, -0.1851],\n",
      "         [ 0.0316,  0.5984,  0.5364,  0.1942,  0.1897, -0.1705],\n",
      "         [ 0.0758,  0.6148,  0.4682,  0.2526,  0.2154, -0.0963],\n",
      "         [ 0.0707,  0.6636,  0.4533,  0.2856,  0.2013, -0.0392],\n",
      "         [ 0.0984,  0.6533,  0.4289,  0.2972,  0.2215, -0.0311]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "mha=MultiHeadAttentionWrapper(in_d,out_d,context_length,0.0,num_head=2)\n",
    "\n",
    "context_vec=mha(batch)\n",
    "print('context_veccccccccc',context_vec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHead attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,num_heads,dropout_percent):\n",
    "        super().__init__()\n",
    "        assert (d_out%num_heads==0)\n",
    "\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=self.d_out//self.num_heads\n",
    "        self.wk=torch.nn.Linear(d_in,d_out)\n",
    "        self.wq=torch.nn.Linear(d_in,d_out)\n",
    "        self.wv=torch.nn.Linear(d_in,d_out)\n",
    "        self.out_proj=torch.nn.Linear(d_out,d_out)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    def forward(self,batch):\n",
    "        b,num_token,d_in=batch.shape\n",
    "        keys=self.wk(batch)# shape: (b,num_token,d_out)\n",
    "        queries=self.wq(batch)\n",
    "        values=self.wv(batch)\n",
    "        \n",
    "        #implicitly split the matrix by adding 'num_heads' dimension\n",
    "        #(b,num_token,d_out)--->(b,num_token,num_head,head_dim)\n",
    "\n",
    "        keys=keys.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        queries=queries.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        values=values.view(b,num_token,self.num_heads,self.head_dim)\n",
    "\n",
    "        #transpose: (b,num_token,num_head,head_dim)---->(b,num_head,num_token,head_dim)\n",
    "\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "\n",
    "      \n",
    "        \n",
    "        #compute self attention with causal mask\n",
    "        attn_score=queries@keys.transpose(2,3) #dot product for each head\n",
    "        mask_bool=self.mask.bool()[:num_token,:num_token]\n",
    "   \n",
    "        attn_score.masked_fill_(mask_bool,-torch.inf)\n",
    "  \n",
    "        attn_weight=torch.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        #shape (b,num_head,num_token,head_dim)---->(b,num_token,num_head,head_dim)\n",
    "        context_vec=(attn_weight@values).transpose(1,2)\n",
    "\n",
    "        #combine heads, where self.d_out=self.head_num*self.head_dim\n",
    "        context_vec=context_vec.contiguous().view(b,num_token,self.d_out)\n",
    "\n",
    "        \n",
    "\n",
    "        print(context_vec)\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000, -0.7733,  1.3123, -1.0517],\n",
      "         [ 0.0093, -0.4044, -0.0967, -0.5161,  0.8724, -0.7013],\n",
      "         [-0.0227, -0.5614, -0.1087, -0.2413,  0.2450, -0.2055],\n",
      "         [-0.0786, -0.4241, -0.2329, -0.3155,  0.5353, -0.4302],\n",
      "         [-0.0480, -0.4746, -0.2006, -0.1963,  0.3463, -0.2713]],\n",
      "\n",
      "        [[-0.1237, -0.9209, -0.1513, -0.9943,  1.0095, -0.8469],\n",
      "         [-0.0628, -0.4680, -0.0769, -0.2872,  0.8188, -0.6376],\n",
      "         [-0.0209, -1.1272, -0.2364,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0074, -0.3185, -0.0762, -0.2956,  0.5209, -0.4081],\n",
      "         [-0.0538, -0.2397, -0.2026, -0.1636,  0.5575, -0.4257],\n",
      "         [-0.0306, -0.2263, -0.1261, -0.2258,  0.7198, -0.5535]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "MHA=MultiHeadAttention(3,6,6,2,0.5)\n",
    "\n",
    "print(batch.shape)\n",
    "MHA(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
