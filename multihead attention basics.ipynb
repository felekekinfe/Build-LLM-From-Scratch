{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3124, -0.5196],\n",
      "        [-0.6133, -0.4779],\n",
      "        [-0.6110, -0.4712],\n",
      "        [-0.3421, -0.2364],\n",
      "        [-0.3979, -0.2197],\n",
      "        [-0.3861, -0.3193]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2088, -0.3265, -0.3210, -0.1877, -0.1310, -0.2528],\n",
      "        [-0.2887, -0.3284, -0.3266, -0.1654, -0.2030, -0.1952],\n",
      "        [-0.2866, -0.3244, -0.3227, -0.1630, -0.2018, -0.1918],\n",
      "        [-0.1543, -0.1658, -0.1653, -0.0809, -0.1103, -0.0921],\n",
      "        [-0.1671, -0.1610, -0.1613, -0.0734, -0.1227, -0.0765],\n",
      "        [-0.1859, -0.2174, -0.2160, -0.1110, -0.1297, -0.1331]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1699, 0.1564, 0.1570, 0.1725, 0.1795, 0.1647],\n",
      "        [0.1621, 0.1576, 0.1578, 0.1769, 0.1723, 0.1732],\n",
      "        [0.1621, 0.1578, 0.1580, 0.1769, 0.1721, 0.1733],\n",
      "        [0.1636, 0.1622, 0.1623, 0.1723, 0.1687, 0.1709],\n",
      "        [0.1619, 0.1626, 0.1626, 0.1730, 0.1671, 0.1727],\n",
      "        [0.1642, 0.1606, 0.1608, 0.1731, 0.1709, 0.1704]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1699, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1621, 0.1576, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1621, 0.1578, 0.1580, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1622, 0.1623, 0.1723, 0.0000, 0.0000],\n",
      "        [0.1619, 0.1626, 0.1626, 0.1730, 0.1671, 0.0000],\n",
      "        [0.1642, 0.1606, 0.1608, 0.1731, 0.1709, 0.1704]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1699],\n",
      "        [0.3198],\n",
      "        [0.4778],\n",
      "        [0.6604],\n",
      "        [0.8273],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5070, 0.4930, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3392, 0.3302, 0.3306, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2477, 0.2457, 0.2458, 0.2609, 0.0000, 0.0000],\n",
      "        [0.1957, 0.1966, 0.1965, 0.2091, 0.2020, 0.0000],\n",
      "        [0.1642, 0.1606, 0.1608, 0.1731, 0.1709, 0.1704]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4517, -0.2462],\n",
      "        [ 0.3241, -0.0025],\n",
      "        [ 0.2786,  0.0797],\n",
      "        [ 0.2224,  0.1132],\n",
      "        [ 0.1822,  0.1180],\n",
      "        [ 0.1739,  0.1360]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2088, -0.3265, -0.3210, -0.1877, -0.1310, -0.2528],\n",
      "        [-0.2887, -0.3284, -0.3266, -0.1654, -0.2030, -0.1952],\n",
      "        [-0.2866, -0.3244, -0.3227, -0.1630, -0.2018, -0.1918],\n",
      "        [-0.1543, -0.1658, -0.1653, -0.0809, -0.1103, -0.0921],\n",
      "        [-0.1671, -0.1610, -0.1613, -0.0734, -0.1227, -0.0765],\n",
      "        [-0.1859, -0.2174, -0.2160, -0.1110, -0.1297, -0.1331]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2088,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2887, -0.3284,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2866, -0.3244, -0.3227,    -inf,    -inf,    -inf],\n",
      "        [-0.1543, -0.1658, -0.1653, -0.0809,    -inf,    -inf],\n",
      "        [-0.1671, -0.1610, -0.1613, -0.0734, -0.1227,    -inf],\n",
      "        [-0.1859, -0.2174, -0.2160, -0.1110, -0.1297, -0.1331]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5050, 0.4950, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3375, 0.3311, 0.3314, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2484, 0.2470, 0.2470, 0.2577, 0.0000, 0.0000],\n",
      "        [0.1970, 0.1976, 0.1976, 0.2064, 0.2014, 0.0000],\n",
      "        [0.1649, 0.1624, 0.1625, 0.1712, 0.1696, 0.1693]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3399, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3157, 0.0000, 0.3445, 0.3464],\n",
      "        [0.0000, 0.3156, 0.3159, 0.3537, 0.3441, 0.3466],\n",
      "        [0.0000, 0.0000, 0.3246, 0.3445, 0.0000, 0.3418],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3342, 0.3453],\n",
      "        [0.3284, 0.3212, 0.0000, 0.3463, 0.3417, 0.3409]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal attention Class with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.transpose(1,2))#change row(1) with colmn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        \n",
    "        b,num_token,d_in=input_embedding.shape\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        print(keys.shape[-1])\n",
    "\n",
    "        attn_score=queries@keys.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
    "        \n",
    "        print('attn_score',attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "        print('attn_weight',attn_weight)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        masked_context_vectore=attn_weight@values\n",
    "        return masked_context_vectore\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "in_d,out_d=3,3\n",
    "print(in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Causal_Attention(\n",
      "  (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "3\n",
      "attn_score tensor([[[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]],\n",
      "\n",
      "        [[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0755,  0.2528,  0.4010],\n",
       "         [ 0.0065,  0.5118,  0.5036],\n",
       "         [ 0.0316,  0.5984,  0.5364],\n",
       "         [ 0.0758,  0.6148,  0.4682],\n",
       "         [ 0.0707,  0.6636,  0.4533],\n",
       "         [ 0.0984,  0.6533,  0.4289]],\n",
       "\n",
       "        [[-0.0755,  0.2528,  0.4010],\n",
       "         [ 0.0065,  0.5118,  0.5036],\n",
       "         [ 0.0316,  0.5984,  0.5364],\n",
       "         [ 0.0758,  0.6148,  0.4682],\n",
       "         [ 0.0707,  0.6636,  0.4533],\n",
       "         [ 0.0984,  0.6533,  0.4289]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "ca=Causal_Attention(in_d,out_d,context_length,0.0)\n",
    "print(ca)\n",
    "\n",
    "ca.forward(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi head attention wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent,num_head):\n",
    "\n",
    "        super().__init__()\n",
    "        self.heads=torch.nn.ModuleList([Causal_Attention(in_d,out_d,context_length,dropout_percent) for _ in range(num_head)])\n",
    "        print('self head',self.heads)\n",
    "    def forward(self,inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "self head ModuleList(\n",
      "  (0-1): 2 x Causal_Attention(\n",
      "    (w_q): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (w_k): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (w_v): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "3\n",
      "attn_score tensor([[[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]],\n",
      "\n",
      "        [[-0.2173,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6607, -0.5641,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.6516, -0.5573, -0.5477,    -inf,    -inf,    -inf],\n",
      "         [-0.3954, -0.3047, -0.3003, -0.2375,    -inf,    -inf],\n",
      "         [-0.2972, -0.2652, -0.2612, -0.2145, -0.1605,    -inf],\n",
      "         [-0.5065, -0.3916, -0.3856, -0.3170, -0.2411, -0.3807]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4861, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3207, 0.3387, 0.3406, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2378, 0.2506, 0.2512, 0.2605, 0.0000, 0.0000],\n",
      "         [0.1934, 0.1970, 0.1975, 0.2029, 0.2093, 0.0000],\n",
      "         [0.1539, 0.1645, 0.1650, 0.1717, 0.1794, 0.1655]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "3\n",
      "attn_score tensor([[[-0.7599,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1980, -1.5578,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1836, -1.5390, -1.5286,    -inf,    -inf,    -inf],\n",
      "         [-0.7215, -0.9420, -0.9343, -0.6817,    -inf,    -inf],\n",
      "         [-0.6178, -0.8005, -0.7966, -0.5784, -0.5878,    -inf],\n",
      "         [-0.8995, -1.1750, -1.1648, -0.8494, -0.7740, -0.9742]],\n",
      "\n",
      "        [[-0.7599,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1980, -1.5578,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1836, -1.5390, -1.5286,    -inf,    -inf,    -inf],\n",
      "         [-0.7215, -0.9420, -0.9343, -0.6817,    -inf,    -inf],\n",
      "         [-0.6178, -0.8005, -0.7966, -0.5784, -0.5878,    -inf],\n",
      "         [-0.8995, -1.1750, -1.1648, -0.8494, -0.7740, -0.9742]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3797, 0.3092, 0.3111, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2640, 0.2324, 0.2335, 0.2701, 0.0000, 0.0000],\n",
      "         [0.2065, 0.1858, 0.1863, 0.2113, 0.2101, 0.0000],\n",
      "         [0.1732, 0.1477, 0.1486, 0.1783, 0.1862, 0.1659]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3797, 0.3092, 0.3111, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2640, 0.2324, 0.2335, 0.2701, 0.0000, 0.0000],\n",
      "         [0.2065, 0.1858, 0.1863, 0.2113, 0.2101, 0.0000],\n",
      "         [0.1732, 0.1477, 0.1486, 0.1783, 0.1862, 0.1659]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_veccccccccc tensor([[[-0.0755,  0.2528,  0.4010, -0.1045,  0.3258, -0.2155],\n",
      "         [ 0.0065,  0.5118,  0.5036,  0.1106,  0.2283, -0.1851],\n",
      "         [ 0.0316,  0.5984,  0.5364,  0.1942,  0.1897, -0.1705],\n",
      "         [ 0.0758,  0.6148,  0.4682,  0.2526,  0.2154, -0.0963],\n",
      "         [ 0.0707,  0.6636,  0.4533,  0.2856,  0.2013, -0.0392],\n",
      "         [ 0.0984,  0.6533,  0.4289,  0.2972,  0.2215, -0.0311]],\n",
      "\n",
      "        [[-0.0755,  0.2528,  0.4010, -0.1045,  0.3258, -0.2155],\n",
      "         [ 0.0065,  0.5118,  0.5036,  0.1106,  0.2283, -0.1851],\n",
      "         [ 0.0316,  0.5984,  0.5364,  0.1942,  0.1897, -0.1705],\n",
      "         [ 0.0758,  0.6148,  0.4682,  0.2526,  0.2154, -0.0963],\n",
      "         [ 0.0707,  0.6636,  0.4533,  0.2856,  0.2013, -0.0392],\n",
      "         [ 0.0984,  0.6533,  0.4289,  0.2972,  0.2215, -0.0311]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "mha=MultiHeadAttentionWrapper(in_d,out_d,context_length,0.0,num_head=2)\n",
    "\n",
    "context_vec=mha(batch)\n",
    "print('context_veccccccccc',context_vec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHead attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
