{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0929,  0.3304],\n",
      "        [-0.1690,  0.6231],\n",
      "        [-0.1644,  0.6046],\n",
      "        [-0.1047,  0.3915],\n",
      "        [-0.0342,  0.1024],\n",
      "        [-0.1554,  0.5903]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1286, -0.0743, -0.0696, -0.0397,  0.0348, -0.0882],\n",
      "        [-0.2442, -0.1441, -0.1352, -0.0771,  0.0634, -0.1692],\n",
      "        [-0.2369, -0.1397, -0.1310, -0.0747,  0.0617, -0.1640],\n",
      "        [-0.1538, -0.0916, -0.0859, -0.0490,  0.0393, -0.1070],\n",
      "        [-0.0384, -0.0195, -0.0181, -0.0102,  0.0127, -0.0248],\n",
      "        [-0.2326, -0.1397, -0.1312, -0.0749,  0.0584, -0.1625]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1588, 0.1650, 0.1655, 0.1691, 0.1782, 0.1634],\n",
      "        [0.1521, 0.1632, 0.1642, 0.1711, 0.1890, 0.1603],\n",
      "        [0.1525, 0.1633, 0.1643, 0.1710, 0.1883, 0.1605],\n",
      "        [0.1575, 0.1645, 0.1652, 0.1696, 0.1805, 0.1628],\n",
      "        [0.1641, 0.1663, 0.1665, 0.1674, 0.1701, 0.1657],\n",
      "        [0.1529, 0.1633, 0.1643, 0.1710, 0.1879, 0.1607]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1521, 0.1632, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1525, 0.1633, 0.1643, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1575, 0.1645, 0.1652, 0.1696, 0.0000, 0.0000],\n",
      "        [0.1641, 0.1663, 0.1665, 0.1674, 0.1701, 0.0000],\n",
      "        [0.1529, 0.1633, 0.1643, 0.1710, 0.1879, 0.1607]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1588],\n",
      "        [0.3153],\n",
      "        [0.4801],\n",
      "        [0.6568],\n",
      "        [0.8343],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4823, 0.5177, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3176, 0.3402, 0.3423, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2397, 0.2505, 0.2515, 0.2582, 0.0000, 0.0000],\n",
      "        [0.1967, 0.1993, 0.1995, 0.2006, 0.2039, 0.0000],\n",
      "        [0.1529, 0.1633, 0.1643, 0.1710, 0.1879, 0.1607]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1567,  0.4693],\n",
      "        [-0.2068,  0.5076],\n",
      "        [-0.2236,  0.5170],\n",
      "        [-0.1982,  0.4582],\n",
      "        [-0.2049,  0.4137],\n",
      "        [-0.1904,  0.4051]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1286, -0.0743, -0.0696, -0.0397,  0.0348, -0.0882],\n",
      "        [-0.2442, -0.1441, -0.1352, -0.0771,  0.0634, -0.1692],\n",
      "        [-0.2369, -0.1397, -0.1310, -0.0747,  0.0617, -0.1640],\n",
      "        [-0.1538, -0.0916, -0.0859, -0.0490,  0.0393, -0.1070],\n",
      "        [-0.0384, -0.0195, -0.0181, -0.0102,  0.0127, -0.0248],\n",
      "        [-0.2326, -0.1397, -0.1312, -0.0749,  0.0584, -0.1625]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0.1286,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2442, -0.1441,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.2369, -0.1397, -0.1310,    -inf,    -inf,    -inf],\n",
      "        [-0.1538, -0.0916, -0.0859, -0.0490,    -inf,    -inf],\n",
      "        [-0.0384, -0.0195, -0.0181, -0.0102,  0.0127,    -inf],\n",
      "        [-0.2326, -0.1397, -0.1312, -0.0749,  0.0584, -0.1625]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4875, 0.5125, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3221, 0.3382, 0.3397, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2427, 0.2504, 0.2511, 0.2558, 0.0000, 0.0000],\n",
      "        [0.1976, 0.1995, 0.1997, 0.2004, 0.2028, 0.0000],\n",
      "        [0.1569, 0.1644, 0.1651, 0.1698, 0.1815, 0.1625]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3285, 0.0000, 0.3780, 0.3207],\n",
      "        [0.0000, 0.3267, 0.3287, 0.3420, 0.3766, 0.3211],\n",
      "        [0.0000, 0.0000, 0.3304, 0.3391, 0.0000, 0.3255],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3402, 0.3313],\n",
      "        [0.3058, 0.3266, 0.0000, 0.3419, 0.3757, 0.3214]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal attention Class with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.transpose(1,2))#change row(1) with colmn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        \n",
    "        b,num_token,d_in=input_embedding.shape\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        print(keys.shape[-1])\n",
    "\n",
    "        attn_score=queries@keys.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
    "        \n",
    "        print('attn_score',attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/3**0.5,dim=-1)\n",
    "        print('attn_weight',attn_weight)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        masked_context_vectore=attn_weight@values\n",
    "        return masked_context_vectore\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "in_d,out_d=3,2\n",
    "print(in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Causal_Attention(\n",
      "  (w_q): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (w_k): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (w_v): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "2\n",
      "attn_score tensor([[[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]],\n",
      "\n",
      "        [[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5028, 0.4972, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3363, 0.3323, 0.3315, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2529, 0.2573, 0.2562, 0.2336, 0.0000, 0.0000],\n",
      "         [0.2090, 0.2055, 0.2052, 0.1894, 0.1910, 0.0000],\n",
      "         [0.1720, 0.1759, 0.1751, 0.1586, 0.1511, 0.1674]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5028, 0.4972, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3363, 0.3323, 0.3315, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2529, 0.2573, 0.2562, 0.2336, 0.0000, 0.0000],\n",
      "         [0.2090, 0.2055, 0.2052, 0.1894, 0.1910, 0.0000],\n",
      "         [0.1720, 0.1759, 0.1751, 0.1586, 0.1511, 0.1674]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0960,  0.7940],\n",
       "         [ 0.0286,  0.9389],\n",
       "         [ 0.0659,  0.9852],\n",
       "         [ 0.1066,  0.9598],\n",
       "         [ 0.0661,  0.9304],\n",
       "         [ 0.1183,  0.9366]],\n",
       "\n",
       "        [[-0.0960,  0.7940],\n",
       "         [ 0.0286,  0.9389],\n",
       "         [ 0.0659,  0.9852],\n",
       "         [ 0.1066,  0.9598],\n",
       "         [ 0.0661,  0.9304],\n",
       "         [ 0.1183,  0.9366]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "ca=Causal_Attention(in_d,out_d,context_length,0.0)\n",
    "print(ca)\n",
    "\n",
    "ca.forward(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi head attention wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent,num_head):\n",
    "\n",
    "        super().__init__()\n",
    "        self.heads=torch.nn.ModuleList([Causal_Attention(in_d,out_d,context_length,dropout_percent) for _ in range(num_head)])\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n",
      "attn_score tensor([[[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]],\n",
      "\n",
      "        [[0.4271,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5730, 0.5534,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.5703, 0.5495, 0.5453,   -inf,   -inf,   -inf],\n",
      "         [0.5143, 0.5440, 0.5368, 0.3764,   -inf,   -inf],\n",
      "         [0.4756, 0.4463, 0.4436, 0.3051, 0.3194,   -inf],\n",
      "         [0.5452, 0.5841, 0.5760, 0.4047, 0.3213, 0.4979]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5028, 0.4972, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3363, 0.3323, 0.3315, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2529, 0.2573, 0.2562, 0.2336, 0.0000, 0.0000],\n",
      "         [0.2090, 0.2055, 0.2052, 0.1894, 0.1910, 0.0000],\n",
      "         [0.1720, 0.1759, 0.1751, 0.1586, 0.1511, 0.1674]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5028, 0.4972, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3363, 0.3323, 0.3315, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2529, 0.2573, 0.2562, 0.2336, 0.0000, 0.0000],\n",
      "         [0.2090, 0.2055, 0.2052, 0.1894, 0.1910, 0.0000],\n",
      "         [0.1720, 0.1759, 0.1751, 0.1586, 0.1511, 0.1674]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "2\n",
      "attn_score tensor([[[ 0.0191,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.0632, -0.2106,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.0659, -0.2167, -0.2134,    -inf,    -inf,    -inf],\n",
      "         [-0.0236, -0.1298, -0.1288, -0.1702,    -inf,    -inf],\n",
      "         [-0.0898, -0.2728, -0.2681, -0.2246, -0.1373,    -inf],\n",
      "         [-0.0005, -0.0765, -0.0768, -0.1445, -0.1104, -0.1375]],\n",
      "\n",
      "        [[ 0.0191,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.0632, -0.2106,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-0.0659, -0.2167, -0.2134,    -inf,    -inf,    -inf],\n",
      "         [-0.0236, -0.1298, -0.1288, -0.1702,    -inf,    -inf],\n",
      "         [-0.0898, -0.2728, -0.2681, -0.2246, -0.1373,    -inf],\n",
      "         [-0.0005, -0.0765, -0.0768, -0.1445, -0.1104, -0.1375]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5213, 0.4787, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3527, 0.3233, 0.3239, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2631, 0.2475, 0.2476, 0.2418, 0.0000, 0.0000],\n",
      "         [0.2128, 0.1914, 0.1919, 0.1968, 0.2070, 0.0000],\n",
      "         [0.1755, 0.1680, 0.1680, 0.1615, 0.1647, 0.1622]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5213, 0.4787, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3527, 0.3233, 0.3239, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2631, 0.2475, 0.2476, 0.2418, 0.0000, 0.0000],\n",
      "         [0.2128, 0.1914, 0.1919, 0.1968, 0.2070, 0.0000],\n",
      "         [0.1755, 0.1680, 0.1680, 0.1615, 0.1647, 0.1622]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_veccccccccc tensor([[[-0.0960,  0.7940, -0.2296,  0.3355],\n",
      "         [ 0.0286,  0.9389, -0.3367,  0.3492],\n",
      "         [ 0.0659,  0.9852, -0.3728,  0.3577],\n",
      "         [ 0.1066,  0.9598, -0.3587,  0.3175],\n",
      "         [ 0.0661,  0.9304, -0.3358,  0.3547],\n",
      "         [ 0.1183,  0.9366, -0.3444,  0.3116]],\n",
      "\n",
      "        [[-0.0960,  0.7940, -0.2296,  0.3355],\n",
      "         [ 0.0286,  0.9389, -0.3367,  0.3492],\n",
      "         [ 0.0659,  0.9852, -0.3728,  0.3577],\n",
      "         [ 0.1066,  0.9598, -0.3587,  0.3175],\n",
      "         [ 0.0661,  0.9304, -0.3358,  0.3547],\n",
      "         [ 0.1183,  0.9366, -0.3444,  0.3116]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "mha=MultiHeadAttentionWrapper(in_d,out_d,context_length,0.0,num_head=2)\n",
    "\n",
    "context_vec=mha(batch)\n",
    "print('context_veccccccccc',context_vec)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
