{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=2, bias=False)\n"
     ]
    }
   ],
   "source": [
    "w_q=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "print(w_q)\n",
    "w_k=torch.nn.Linear(inputs.shape[1],2,bias=False)\n",
    "w_v=torch.nn.Linear(inputs.shape[1],2,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0903, -0.1535],\n",
      "        [ 0.2760, -0.3187],\n",
      "        [ 0.2615, -0.3105],\n",
      "        [ 0.2004, -0.1990],\n",
      "        [-0.0732, -0.0765],\n",
      "        [ 0.3485, -0.2882]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.0903,  0.2760,  0.2615,  0.2004, -0.0732,  0.3485],\n",
      "        [-0.1535, -0.3187, -0.3105, -0.1990, -0.0765, -0.2882]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=w_q(inputs)\n",
    "keys=w_k(inputs)\n",
    "values=w_v(inputs)\n",
    "print(queries)\n",
    "print(queries.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0232, -0.0178, -0.0144, -0.0245,  0.0502, -0.0569],\n",
      "        [ 0.0096, -0.0810, -0.0726, -0.0763,  0.0983, -0.1585],\n",
      "        [ 0.0126, -0.0752, -0.0672, -0.0722,  0.0963, -0.1511],\n",
      "        [-0.0063, -0.0646, -0.0589, -0.0557,  0.0595, -0.1118],\n",
      "        [ 0.0631,  0.0501,  0.0499,  0.0219,  0.0331,  0.0255],\n",
      "        [-0.0344, -0.1225, -0.1134, -0.0974,  0.0822, -0.1884]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_score=queries@keys.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1702, 0.1653, 0.1657, 0.1645, 0.1735, 0.1608],\n",
      "        [0.1731, 0.1624, 0.1634, 0.1630, 0.1844, 0.1537],\n",
      "        [0.1731, 0.1626, 0.1636, 0.1630, 0.1836, 0.1541],\n",
      "        [0.1705, 0.1636, 0.1643, 0.1647, 0.1786, 0.1583],\n",
      "        [0.1693, 0.1678, 0.1678, 0.1645, 0.1658, 0.1649],\n",
      "        [0.1717, 0.1613, 0.1624, 0.1642, 0.1864, 0.1540]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weight=torch.softmax(attn_score/keys.shape[1]**0.5,dim=1)\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#use pytorch tril function to diagonalize\n",
    "context_length=inputs.shape[0]\n",
    "mask1=torch.tril(torch.ones(context_length,context_length))\n",
    "print(mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1784, 0.1528, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1782, 0.1531, 0.1539, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1733, 0.1579, 0.1584, 0.1699, 0.0000, 0.0000],\n",
      "        [0.1708, 0.1637, 0.1639, 0.1669, 0.1702, 0.0000],\n",
      "        [0.1759, 0.1539, 0.1546, 0.1716, 0.1816, 0.1624]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#use element wise multiplication not matrix\n",
    "masked_attn_score=attn_weight*mask1\n",
    "print(masked_attn_score)\n",
    "\n",
    "#but this attention score is not fully masked or optimal becaus the softmax normalization adds influence on its from the previous attn score so use upper negative infinity mask instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1751],\n",
      "        [0.3312],\n",
      "        [0.4853],\n",
      "        [0.6596],\n",
      "        [0.8354],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5387, 0.4613, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3672, 0.3156, 0.3172, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2628, 0.2394, 0.2402, 0.2576, 0.0000, 0.0000],\n",
      "        [0.2045, 0.1959, 0.1961, 0.1997, 0.2038, 0.0000],\n",
      "        [0.1759, 0.1539, 0.1546, 0.1716, 0.1816, 0.1624]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#normalize the masked attention score  to sum up 1 in each row to get masked attn weight\n",
    "row_sum=masked_attn_score.sum(dim=1,keepdim=True)\n",
    "print(row_sum)\n",
    "masked_attn_weight=masked_attn_score/row_sum\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2130, -0.6795],\n",
      "        [ 0.3920, -0.8111],\n",
      "        [ 0.4560, -0.8569],\n",
      "        [ 0.4368, -0.7743],\n",
      "        [ 0.4082, -0.7247],\n",
      "        [ 0.4111, -0.7052]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_context_vectore=masked_attn_weight@values\n",
    "print(masked_context_vectore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative infinity masking to avoid any influence of future attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0588, -0.1890, -0.1850, -0.1207, -0.0614, -0.1651],\n",
      "        [-0.1896, -0.4089, -0.4014, -0.2481, -0.1534, -0.3344],\n",
      "        [-0.1833, -0.3974, -0.3901, -0.2414, -0.1488, -0.3254],\n",
      "        [-0.1276, -0.2595, -0.2549, -0.1559, -0.0997, -0.2095],\n",
      "        [-0.0189, -0.0796, -0.0778, -0.0520, -0.0239, -0.0717],\n",
      "        [-0.1940, -0.3834, -0.3766, -0.2293, -0.1491, -0.3075]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[ 0.0232,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0096, -0.0810,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0126, -0.0752, -0.0672,    -inf,    -inf,    -inf],\n",
      "        [-0.0063, -0.0646, -0.0589, -0.0557,    -inf,    -inf],\n",
      "        [ 0.0631,  0.0501,  0.0499,  0.0219,  0.0331,    -inf],\n",
      "        [-0.0344, -0.1225, -0.1134, -0.0974,  0.0822, -0.1884]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "print(mask)\n",
    "masked=attn_score.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5274, 0.4726, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3571, 0.3209, 0.3220, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2590, 0.2425, 0.2431, 0.2554, 0.0000, 0.0000],\n",
      "        [0.2032, 0.1971, 0.1973, 0.1998, 0.2027, 0.0000],\n",
      "        [0.1732, 0.1576, 0.1581, 0.1702, 0.1772, 0.1637]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_weight=torch.softmax(masked/keys.shape[1],dim=1)\n",
    "print(masked_attn_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking additional attention weight with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.5, inplace=False)\n",
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout)\n",
    "ones=torch.ones(6,6)\n",
    "print(ones)\n",
    "print(dropout(ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3503, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3072, 0.0000, 0.3661, 0.3221],\n",
      "        [0.0000, 0.3063, 0.3079, 0.3420, 0.3652, 0.3223],\n",
      "        [0.0000, 0.0000, 0.3169, 0.3398, 0.0000, 0.3272],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3404, 0.3291],\n",
      "        [0.3519, 0.3078, 0.0000, 0.3432, 0.3632, 0.3247]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_dropout=dropout(attn_weight)\n",
    "print(attn_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal attention Class with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch.transpose(1,2))#change row(1) with colmn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Causal_Attention(torch.nn.Module):\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent):\n",
    "        super().__init__()\n",
    "        self.w_q=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_k=torch.nn.Linear(in_d,out_d)\n",
    "        self.w_v=torch.nn.Linear(in_d,out_d)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,input_embedding):\n",
    "        \n",
    "        b,num_token,d_in=input_embedding.shape\n",
    "        queries=self.w_q(input_embedding)\n",
    "        keys=self.w_k(input_embedding)\n",
    "        values=self.w_v(input_embedding)\n",
    "\n",
    "        print(keys.shape[-1])\n",
    "\n",
    "        attn_score=queries@keys.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:context_length, :context_length],-torch.inf)\n",
    "        \n",
    "        print('attn_score',attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "        print('attn_weight',attn_weight)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        masked_context_vectore=attn_weight@values\n",
    "        return masked_context_vectore\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "in_d,out_d=3,3\n",
    "print(in_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Causal_Attention(\n",
      "  (w_q): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (w_k): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (w_v): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "6\n",
      "attn_score tensor([[[0.3992,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3519, 0.3963,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3568, 0.4077, 0.4128,   -inf,   -inf,   -inf],\n",
      "         [0.1898, 0.2388, 0.2436, 0.1718,   -inf,   -inf],\n",
      "         [0.3699, 0.5750, 0.5811, 0.4127, 0.5952,   -inf],\n",
      "         [0.1539, 0.1378, 0.1424, 0.1046, 0.2145, 0.0622]],\n",
      "\n",
      "        [[0.3992,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3519, 0.3963,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3568, 0.4077, 0.4128,   -inf,   -inf,   -inf],\n",
      "         [0.1898, 0.2388, 0.2436, 0.1718,   -inf,   -inf],\n",
      "         [0.3699, 0.5750, 0.5811, 0.4127, 0.5952,   -inf],\n",
      "         [0.1539, 0.1378, 0.1424, 0.1046, 0.2145, 0.0622]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3285, 0.3354, 0.3361, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2478, 0.2528, 0.2533, 0.2460, 0.0000, 0.0000],\n",
      "         [0.1890, 0.2055, 0.2060, 0.1923, 0.2072, 0.0000],\n",
      "         [0.1679, 0.1668, 0.1671, 0.1645, 0.1721, 0.1617]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3285, 0.3354, 0.3361, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2478, 0.2528, 0.2533, 0.2460, 0.0000, 0.0000],\n",
      "         [0.1890, 0.2055, 0.2060, 0.1923, 0.2072, 0.0000],\n",
      "         [0.1679, 0.1668, 0.1671, 0.1645, 0.1721, 0.1617]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3219,  0.2094, -0.9932,  0.0712,  0.1319,  0.1719],\n",
       "         [ 0.4575,  0.2406, -1.1512,  0.1678,  0.3740,  0.0621],\n",
       "         [ 0.5021,  0.2493, -1.1980,  0.2061,  0.4552,  0.0246],\n",
       "         [ 0.4403,  0.2167, -1.1513,  0.2155,  0.4965,  0.0534],\n",
       "         [ 0.4287,  0.1912, -1.0795,  0.2991,  0.5302,  0.0399],\n",
       "         [ 0.4024,  0.1877, -1.0947,  0.2588,  0.5358,  0.0623]],\n",
       "\n",
       "        [[ 0.3219,  0.2094, -0.9932,  0.0712,  0.1319,  0.1719],\n",
       "         [ 0.4575,  0.2406, -1.1512,  0.1678,  0.3740,  0.0621],\n",
       "         [ 0.5021,  0.2493, -1.1980,  0.2061,  0.4552,  0.0246],\n",
       "         [ 0.4403,  0.2167, -1.1513,  0.2155,  0.4965,  0.0534],\n",
       "         [ 0.4287,  0.1912, -1.0795,  0.2991,  0.5302,  0.0399],\n",
       "         [ 0.4024,  0.1877, -1.0947,  0.2588,  0.5358,  0.0623]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "ca=Causal_Attention(in_d,6,context_length,0.0)\n",
    "print(ca)\n",
    "\n",
    "ca.forward(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi head attention wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,in_d,out_d,context_length,dropout_percent,num_head):\n",
    "\n",
    "        super().__init__()\n",
    "        self.heads=torch.nn.ModuleList([Causal_Attention(in_d,out_d,context_length,dropout_percent) for _ in range(num_head)])\n",
    "        print('self head',self.heads)\n",
    "    def forward(self,inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "self head ModuleList(\n",
      "  (0-1): 2 x Causal_Attention(\n",
      "    (w_q): Linear(in_features=3, out_features=6, bias=True)\n",
      "    (w_k): Linear(in_features=3, out_features=6, bias=True)\n",
      "    (w_v): Linear(in_features=3, out_features=6, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "6\n",
      "attn_score tensor([[[0.3992,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3519, 0.3963,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3568, 0.4077, 0.4128,   -inf,   -inf,   -inf],\n",
      "         [0.1898, 0.2388, 0.2436, 0.1718,   -inf,   -inf],\n",
      "         [0.3699, 0.5750, 0.5811, 0.4127, 0.5952,   -inf],\n",
      "         [0.1539, 0.1378, 0.1424, 0.1046, 0.2145, 0.0622]],\n",
      "\n",
      "        [[0.3992,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3519, 0.3963,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.3568, 0.4077, 0.4128,   -inf,   -inf,   -inf],\n",
      "         [0.1898, 0.2388, 0.2436, 0.1718,   -inf,   -inf],\n",
      "         [0.3699, 0.5750, 0.5811, 0.4127, 0.5952,   -inf],\n",
      "         [0.1539, 0.1378, 0.1424, 0.1046, 0.2145, 0.0622]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3285, 0.3354, 0.3361, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2478, 0.2528, 0.2533, 0.2460, 0.0000, 0.0000],\n",
      "         [0.1890, 0.2055, 0.2060, 0.1923, 0.2072, 0.0000],\n",
      "         [0.1679, 0.1668, 0.1671, 0.1645, 0.1721, 0.1617]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3285, 0.3354, 0.3361, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2478, 0.2528, 0.2533, 0.2460, 0.0000, 0.0000],\n",
      "         [0.1890, 0.2055, 0.2060, 0.1923, 0.2072, 0.0000],\n",
      "         [0.1679, 0.1668, 0.1671, 0.1645, 0.1721, 0.1617]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "6\n",
      "attn_score tensor([[[-0.6618,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1062, -1.1947,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.0994, -1.1837, -1.1855,    -inf,    -inf,    -inf],\n",
      "         [-0.8428, -0.8767, -0.8786, -0.3891,    -inf,    -inf],\n",
      "         [-0.7927, -0.7665, -0.7710, -0.2345, -0.5788,    -inf],\n",
      "         [-0.9300, -1.0101, -1.0107, -0.5012, -0.7472, -0.5338]],\n",
      "\n",
      "        [[-0.6618,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.1062, -1.1947,    -inf,    -inf,    -inf,    -inf],\n",
      "         [-1.0994, -1.1837, -1.1855,    -inf,    -inf,    -inf],\n",
      "         [-0.8428, -0.8767, -0.8786, -0.3891,    -inf,    -inf],\n",
      "         [-0.7927, -0.7665, -0.7710, -0.2345, -0.5788,    -inf],\n",
      "         [-0.9300, -1.0101, -1.0107, -0.5012, -0.7472, -0.5338]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weight tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5090, 0.4910, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3411, 0.3296, 0.3293, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2395, 0.2362, 0.2360, 0.2882, 0.0000, 0.0000],\n",
      "         [0.1863, 0.1883, 0.1880, 0.2340, 0.2033, 0.0000],\n",
      "         [0.1567, 0.1517, 0.1517, 0.1867, 0.1689, 0.1843]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5090, 0.4910, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3411, 0.3296, 0.3293, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2395, 0.2362, 0.2360, 0.2882, 0.0000, 0.0000],\n",
      "         [0.1863, 0.1883, 0.1880, 0.2340, 0.2033, 0.0000],\n",
      "         [0.1567, 0.1517, 0.1517, 0.1867, 0.1689, 0.1843]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_veccccccccc tensor([[[ 3.2190e-01,  2.0944e-01, -9.9319e-01,  7.1200e-02,  1.3190e-01,\n",
      "           1.7189e-01,  5.2404e-01, -4.5444e-01,  2.8967e-01,  1.1679e-01,\n",
      "          -2.2618e-02,  4.7939e-01],\n",
      "         [ 4.5751e-01,  2.4060e-01, -1.1512e+00,  1.6784e-01,  3.7402e-01,\n",
      "           6.2144e-02,  5.2176e-01, -2.4266e-01,  3.6640e-01,  4.4069e-02,\n",
      "          -1.6010e-01,  5.4106e-01],\n",
      "         [ 5.0210e-01,  2.4935e-01, -1.1980e+00,  2.0605e-01,  4.5522e-01,\n",
      "           2.4600e-02,  5.1463e-01, -1.7358e-01,  3.9177e-01,  2.2739e-02,\n",
      "          -2.0332e-01,  5.5759e-01],\n",
      "         [ 4.4026e-01,  2.1671e-01, -1.1513e+00,  2.1547e-01,  4.9653e-01,\n",
      "           5.3432e-02,  4.6079e-01, -1.2385e-01,  3.4944e-01, -3.1669e-02,\n",
      "          -1.8352e-01,  5.8204e-01],\n",
      "         [ 4.2868e-01,  1.9121e-01, -1.0795e+00,  2.9915e-01,  5.3017e-01,\n",
      "           3.9879e-02,  3.7189e-01, -1.4898e-01,  3.3339e-01, -2.8477e-04,\n",
      "          -1.3740e-01,  5.3209e-01],\n",
      "         [ 4.0235e-01,  1.8772e-01, -1.0947e+00,  2.5881e-01,  5.3575e-01,\n",
      "           6.2287e-02,  4.0062e-01, -1.0705e-01,  3.2408e-01, -4.6411e-02,\n",
      "          -1.5805e-01,  5.7515e-01]],\n",
      "\n",
      "        [[ 3.2190e-01,  2.0944e-01, -9.9319e-01,  7.1200e-02,  1.3190e-01,\n",
      "           1.7189e-01,  5.2404e-01, -4.5444e-01,  2.8967e-01,  1.1679e-01,\n",
      "          -2.2618e-02,  4.7939e-01],\n",
      "         [ 4.5751e-01,  2.4060e-01, -1.1512e+00,  1.6784e-01,  3.7402e-01,\n",
      "           6.2144e-02,  5.2176e-01, -2.4266e-01,  3.6640e-01,  4.4069e-02,\n",
      "          -1.6010e-01,  5.4106e-01],\n",
      "         [ 5.0210e-01,  2.4935e-01, -1.1980e+00,  2.0605e-01,  4.5522e-01,\n",
      "           2.4600e-02,  5.1463e-01, -1.7358e-01,  3.9177e-01,  2.2739e-02,\n",
      "          -2.0332e-01,  5.5759e-01],\n",
      "         [ 4.4026e-01,  2.1671e-01, -1.1513e+00,  2.1547e-01,  4.9653e-01,\n",
      "           5.3432e-02,  4.6079e-01, -1.2385e-01,  3.4944e-01, -3.1669e-02,\n",
      "          -1.8352e-01,  5.8204e-01],\n",
      "         [ 4.2868e-01,  1.9121e-01, -1.0795e+00,  2.9915e-01,  5.3017e-01,\n",
      "           3.9879e-02,  3.7189e-01, -1.4898e-01,  3.3339e-01, -2.8477e-04,\n",
      "          -1.3740e-01,  5.3209e-01],\n",
      "         [ 4.0235e-01,  1.8772e-01, -1.0947e+00,  2.5881e-01,  5.3575e-01,\n",
      "           6.2287e-02,  4.0062e-01, -1.0705e-01,  3.2408e-01, -4.6411e-02,\n",
      "          -1.5805e-01,  5.7515e-01]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "print(context_length)\n",
    "mha=MultiHeadAttentionWrapper(in_d,6,context_length,0.0,num_head=2)\n",
    "\n",
    "context_vec=mha(batch)\n",
    "print('context_veccccccccc',context_vec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiHead attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,d_in,context_length,d_out,num_heads,dropout_percent):\n",
    "        super().__init__()\n",
    "        assert (d_out%num_heads==0)\n",
    "\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=self.d_out//self.num_heads\n",
    "        self.wk=torch.nn.Linear(d_in,d_out)\n",
    "        self.wq=torch.nn.Linear(d_in,d_out)\n",
    "        self.wv=torch.nn.Linear(d_in,d_out)\n",
    "        self.out_proj=torch.nn.Linear(d_out,d_out)\n",
    "        self.dropout=torch.nn.Dropout(dropout_percent)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    def forward(self,batch):\n",
    "        b,num_token,d_in=batch.shape\n",
    "        keys=self.wk(batch)# shape: (b,num_token,d_out)\n",
    "        queries=self.wq(batch)\n",
    "        values=self.wv(batch)\n",
    "        print(keys)\n",
    "        #implicitly split the matrix by adding 'num_heads' dimension\n",
    "        #(b,num_token,d_out)--->(b,num_token,num_head,head_dim)\n",
    "\n",
    "        keys=keys.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        queries=queries.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        values=values.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        print(keys)\n",
    "        #transpose: (b,num_token,num_head,head_dim)---->(b,num_head,num_token,head_dim)\n",
    "\n",
    "        keys=keys.transpose(1,2)\n",
    "        print(keys)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "\n",
    "      \n",
    "        \n",
    "        #compute self attention with causal mask\n",
    "        attn_score=queries@keys.transpose(2,3) #dot product for each head\n",
    "        mask_bool=self.mask.bool()[:num_token,:num_token]\n",
    "   \n",
    "        attn_score.masked_fill_(mask_bool,-torch.inf)\n",
    "  \n",
    "        attn_weight=torch.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "\n",
    "        #shape (b,num_head,num_token,head_dim)---->(b,num_token,num_head,head_dim)\n",
    "        context_vec=(attn_weight@values).transpose(1,2)\n",
    "\n",
    "        #combine heads, where self.d_out=self.head_num*self.head_dim\n",
    "        context_vec=context_vec.contiguous().view(b,num_token,self.d_out)\n",
    "        context_vec=self.out_proj(context_vec)#optional\n",
    "        \n",
    "\n",
    "        print(context_vec)\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[ 0.1627, -0.4244,  0.3030,  1.1157,  0.1149,  0.1361],\n",
      "         [-0.2507, -0.4525,  0.6040,  1.3664,  0.2574, -0.0498],\n",
      "         [-0.2412, -0.4527,  0.6213,  1.3642,  0.2674, -0.0268],\n",
      "         [-0.2099, -0.4540,  0.5598,  0.9916,  0.2676,  0.0684],\n",
      "         [-0.0014, -0.4549,  0.9066,  1.1026,  0.4534,  0.4946],\n",
      "         [-0.3072, -0.4520,  0.3942,  1.0423,  0.1694, -0.1663]],\n",
      "\n",
      "        [[ 0.1627, -0.4244,  0.3030,  1.1157,  0.1149,  0.1361],\n",
      "         [-0.2507, -0.4525,  0.6040,  1.3664,  0.2574, -0.0498],\n",
      "         [-0.2412, -0.4527,  0.6213,  1.3642,  0.2674, -0.0268],\n",
      "         [-0.2099, -0.4540,  0.5598,  0.9916,  0.2676,  0.0684],\n",
      "         [-0.0014, -0.4549,  0.9066,  1.1026,  0.4534,  0.4946],\n",
      "         [-0.3072, -0.4520,  0.3942,  1.0423,  0.1694, -0.1663]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.1627, -0.4244,  0.3030],\n",
      "          [ 1.1157,  0.1149,  0.1361]],\n",
      "\n",
      "         [[-0.2507, -0.4525,  0.6040],\n",
      "          [ 1.3664,  0.2574, -0.0498]],\n",
      "\n",
      "         [[-0.2412, -0.4527,  0.6213],\n",
      "          [ 1.3642,  0.2674, -0.0268]],\n",
      "\n",
      "         [[-0.2099, -0.4540,  0.5598],\n",
      "          [ 0.9916,  0.2676,  0.0684]],\n",
      "\n",
      "         [[-0.0014, -0.4549,  0.9066],\n",
      "          [ 1.1026,  0.4534,  0.4946]],\n",
      "\n",
      "         [[-0.3072, -0.4520,  0.3942],\n",
      "          [ 1.0423,  0.1694, -0.1663]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1627, -0.4244,  0.3030],\n",
      "          [ 1.1157,  0.1149,  0.1361]],\n",
      "\n",
      "         [[-0.2507, -0.4525,  0.6040],\n",
      "          [ 1.3664,  0.2574, -0.0498]],\n",
      "\n",
      "         [[-0.2412, -0.4527,  0.6213],\n",
      "          [ 1.3642,  0.2674, -0.0268]],\n",
      "\n",
      "         [[-0.2099, -0.4540,  0.5598],\n",
      "          [ 0.9916,  0.2676,  0.0684]],\n",
      "\n",
      "         [[-0.0014, -0.4549,  0.9066],\n",
      "          [ 1.1026,  0.4534,  0.4946]],\n",
      "\n",
      "         [[-0.3072, -0.4520,  0.3942],\n",
      "          [ 1.0423,  0.1694, -0.1663]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.1627, -0.4244,  0.3030],\n",
      "          [-0.2507, -0.4525,  0.6040],\n",
      "          [-0.2412, -0.4527,  0.6213],\n",
      "          [-0.2099, -0.4540,  0.5598],\n",
      "          [-0.0014, -0.4549,  0.9066],\n",
      "          [-0.3072, -0.4520,  0.3942]],\n",
      "\n",
      "         [[ 1.1157,  0.1149,  0.1361],\n",
      "          [ 1.3664,  0.2574, -0.0498],\n",
      "          [ 1.3642,  0.2674, -0.0268],\n",
      "          [ 0.9916,  0.2676,  0.0684],\n",
      "          [ 1.1026,  0.4534,  0.4946],\n",
      "          [ 1.0423,  0.1694, -0.1663]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1627, -0.4244,  0.3030],\n",
      "          [-0.2507, -0.4525,  0.6040],\n",
      "          [-0.2412, -0.4527,  0.6213],\n",
      "          [-0.2099, -0.4540,  0.5598],\n",
      "          [-0.0014, -0.4549,  0.9066],\n",
      "          [-0.3072, -0.4520,  0.3942]],\n",
      "\n",
      "         [[ 1.1157,  0.1149,  0.1361],\n",
      "          [ 1.3664,  0.2574, -0.0498],\n",
      "          [ 1.3642,  0.2674, -0.0268],\n",
      "          [ 0.9916,  0.2676,  0.0684],\n",
      "          [ 1.1026,  0.4534,  0.4946],\n",
      "          [ 1.0423,  0.1694, -0.1663]]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-0.1474,  1.3683,  0.8734,  1.0383,  0.5333,  0.5013],\n",
      "         [-0.1319,  0.7335,  0.4969,  0.4357,  0.4576,  0.0215],\n",
      "         [-0.2423,  0.6244,  0.4039,  0.3245,  0.4885, -0.1755],\n",
      "         [-0.0283,  0.8733,  0.5612,  0.5707,  0.4176,  0.2664],\n",
      "         [-0.2707,  0.5715,  0.3900,  0.2052,  0.3259, -0.3219],\n",
      "         [-0.0732,  1.1776,  0.7011,  0.8551,  0.5202,  0.4661]],\n",
      "\n",
      "        [[-0.3988,  0.5019,  0.3301,  0.0829,  0.1989, -0.5142],\n",
      "         [-0.3033,  1.0752,  0.6026,  0.7733,  0.6268,  0.1719],\n",
      "         [-0.1955,  1.5231,  0.8553,  1.2861,  0.8686,  0.7008],\n",
      "         [ 0.0116,  1.3495,  0.8038,  1.0830,  0.6511,  0.7402],\n",
      "         [-0.0797,  0.9453,  0.6028,  0.6078,  0.4145,  0.2173],\n",
      "         [-0.2072,  0.7126,  0.4389,  0.3772,  0.4038, -0.0735]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b,context_length,d_in=batch.shape\n",
    "MHA=MultiHeadAttention(d_in,context_length,d_out=6,num_heads=2,dropout_percent=0.5)\n",
    "\n",
    "print(batch.shape)\n",
    "MHA(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
