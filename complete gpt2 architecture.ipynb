{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    'vocab_size':50257,\n",
    "    'context_length':1024,\n",
    "    'emb_dim':768,\n",
    "    'n_head':12,\n",
    "    'n_layers':12,\n",
    "    'drop_rate':0.1,\n",
    "    'qkv_bias':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,in_dim,out_dim,context_length,drop_rate):\n",
    "        super().__init__()\n",
    "        self.kw=torch.nn.Linear(in_dim,out_dim)\n",
    "        \n",
    "        self.qw=torch.nn.Linear(in_dim,out_dim)\n",
    "        self.vw=torch.nn.Linear(in_dim,out_dim)\n",
    "        self.dropout=torch.nn.Dropout(drop_rate)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,t,in_dim=x.shape\n",
    "\n",
    "        key=self.kw(x)\n",
    "        \n",
    "        query=self.qw(x)\n",
    "        value=self.vw(x)\n",
    "\n",
    "        attn_score=query@key.transpose(1,2)\n",
    "        \n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:t,:t],-torch.inf)\n",
    "        \n",
    "        attn_weight=torch.softmax(attn_score/key.shape[-1]**0.5,dim=-1)\n",
    "        \n",
    "        \n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "        \n",
    "        context_vectore=attn_weight@value\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        return context_vectore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 6])\n",
      "torch.Size([2, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0331, -0.3032, -0.5112,  0.5845,  0.5095, -1.3587,  0.2394,\n",
       "          -0.2758, -1.1549,  0.0226, -0.0384, -0.1624],\n",
       "         [ 0.0256, -0.4098, -0.1742,  0.3135,  0.4450, -0.9321,  0.1538,\n",
       "          -0.1779, -1.0326,  0.0702,  0.1620, -0.2115],\n",
       "         [ 0.0187, -0.3738, -0.2881,  0.3964,  0.4557, -1.0545,  0.0481,\n",
       "          -0.0457, -0.5802,  0.0843,  0.2480, -0.1798],\n",
       "         [ 0.0186, -0.4089, -0.1771,  0.3094,  0.4373, -0.9196,  0.0985,\n",
       "          -0.1061, -0.7383,  0.0696,  0.1775, -0.1781],\n",
       "         [-0.0203, -0.5659, -0.2880,  0.5080,  0.5585, -1.2978,  0.1860,\n",
       "          -0.0441, -0.3022,  0.1267,  0.1170, -0.2173],\n",
       "         [-0.1272, -0.5625, -0.2021,  0.4259,  0.3811, -0.9700,  0.1866,\n",
       "          -0.0515, -0.4178,  0.1388,  0.1679, -0.2466]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0419, -0.5591, -0.4259,  0.6014,  0.6959, -1.6012,  0.1538,\n",
       "          -0.1779, -1.0326,  0.0702,  0.1620, -0.2115],\n",
       "         [ 0.0278, -0.3712, -0.2825,  0.3990,  0.4619, -1.0625,  0.1329,\n",
       "          -0.1435, -0.9895,  0.0923,  0.2344, -0.2373],\n",
       "         [ 0.0210, -0.2790, -0.2140,  0.3015,  0.3480, -0.8016,  0.1411,\n",
       "          -0.1293, -0.9076,  0.1104,  0.2405, -0.2484],\n",
       "         [-0.1388, -0.7450, -0.4037,  0.6026,  0.6118, -1.5010,  0.2033,\n",
       "          -0.0579, -0.4762,  0.1535,  0.1916, -0.2736],\n",
       "         [-0.1284, -0.6248, -0.3408,  0.5021,  0.4992, -1.2390,  0.2362,\n",
       "          -0.1089, -0.7538,  0.1620,  0.2220, -0.3159]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,context_length,drop_rate,num_head):\n",
    "        super().__init__()\n",
    "        self.head=torch.nn.ModuleList([CausalSelfAttention(in_dim,out_dim,context_length,drop_rate) for _ in range(num_head)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "       \n",
    "        return torch.cat([head(x) for head in self.head],dim=-1)\n",
    "    \n",
    "x=MultiHeadAttention(batch.shape[-1],6,batch.shape[1],0.3,2)\n",
    "x(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=torch.nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb=torch.nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb=torch.nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks=torch.nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "\n",
    "        self.final_norm=LayerNorm(cfg['emb_dim'])#implemented\n",
    "        self.output_head=torch.nn.Linear(cfg['emb_dim'], cfg['vocab_size'])\n",
    "\n",
    "    \n",
    "    def forward(self,in_idx):#input ID\n",
    "        batch_size,seq_len=in_idx.shape()\n",
    "        tok_emb=self.tok_emb(in_idx)\n",
    "        pos_emb=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_emb+pos_emb\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)#transformerblock implemented\n",
    "        logits=self.output_head(x)\n",
    "\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
