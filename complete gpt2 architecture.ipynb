{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    'vocab_size':50257,\n",
    "    'context_length':1024,\n",
    "    'emb_dim':768,\n",
    "    'n_head':12,\n",
    "    'n_layers':12,\n",
    "    'drop_rate':0.1,\n",
    "    'qkv_bias':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor(\n",
    "    [\n",
    "     [0.43,0.15,0.89],#your\n",
    "     [0.55,0.87,0.66],#journey\n",
    "     [0.57,0.85,0.64],#starts\n",
    "     [0.22,0.58,0.33],#with\n",
    "     [0.77,0.25,0.10],#one\n",
    "     [0.05,0.80,0.55],#step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=6, bias=True)\n",
      "tensor([[[-0.9083,  0.6658, -0.4917, -0.2189, -0.3603, -0.5090],\n",
      "         [-0.4897,  0.3671, -0.4970, -0.1402, -0.1464, -0.8430],\n",
      "         [-0.4839,  0.3517, -0.4892, -0.1401, -0.1715, -0.8444],\n",
      "         [-0.4565,  0.4113, -0.3878,  0.0363, -0.1990, -0.5902],\n",
      "         [-0.3892,  0.1035, -0.2814, -0.0513, -0.7003, -0.7090],\n",
      "         [-0.5150,  0.5579, -0.4721,  0.0220,  0.0509, -0.5969]],\n",
      "\n",
      "        [[-0.9083,  0.6658, -0.4917, -0.2189, -0.3603, -0.5090],\n",
      "         [-0.4897,  0.3671, -0.4970, -0.1402, -0.1464, -0.8430],\n",
      "         [-0.4839,  0.3517, -0.4892, -0.1401, -0.1715, -0.8444],\n",
      "         [-0.4565,  0.4113, -0.3878,  0.0363, -0.1990, -0.5902],\n",
      "         [-0.3892,  0.1035, -0.2814, -0.0513, -0.7003, -0.7090],\n",
      "         [-0.5150,  0.5579, -0.4721,  0.0220,  0.0509, -0.5969]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.1007,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.4958,  0.0582,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.5008,  0.0704,  0.0716,    -inf,    -inf,    -inf],\n",
      "         [ 0.1533, -0.0983, -0.0977, -0.1154,    -inf,    -inf],\n",
      "         [ 0.3815,  0.2086,  0.2133,  0.1131,  0.2678,    -inf],\n",
      "         [ 0.1321, -0.2049, -0.2062, -0.1756, -0.1855, -0.1669]],\n",
      "\n",
      "        [[ 0.1007,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.4958,  0.0582,    -inf,    -inf,    -inf,    -inf],\n",
      "         [ 0.5008,  0.0704,  0.0716,    -inf,    -inf,    -inf],\n",
      "         [ 0.1533, -0.0983, -0.0977, -0.1154,    -inf,    -inf],\n",
      "         [ 0.3815,  0.2086,  0.2133,  0.1131,  0.2678,    -inf],\n",
      "         [ 0.1321, -0.2049, -0.2062, -0.1756, -0.1855, -0.1669]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5445, 0.4555, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3734, 0.3132, 0.3134, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2702, 0.2438, 0.2439, 0.2421, 0.0000, 0.0000],\n",
      "         [0.2120, 0.1976, 0.1980, 0.1900, 0.2024, 0.0000],\n",
      "         [0.1856, 0.1617, 0.1617, 0.1637, 0.1630, 0.1643]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5445, 0.4555, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3734, 0.3132, 0.3134, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2702, 0.2438, 0.2439, 0.2421, 0.0000, 0.0000],\n",
      "         [0.2120, 0.1976, 0.1980, 0.1900, 0.2024, 0.0000],\n",
      "         [0.1856, 0.1617, 0.1617, 0.1637, 0.1630, 0.1643]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.0650,  0.4974, -1.0731,  0.5541, -0.1693,  0.5412],\n",
      "         [-0.1948, -0.0841, -0.5629,  0.0543, -0.2391,  0.4749],\n",
      "         [-0.1097,  0.1279, -0.7878,  0.2442, -0.2276,  0.5287],\n",
      "         [-0.3105,  0.0014, -1.0834,  0.2069, -0.3874,  0.8901],\n",
      "         [-0.3780, -0.0176, -1.0250,  0.2195, -0.3868,  0.8606],\n",
      "         [-0.3840, -0.1352, -0.8097,  0.0670, -0.3519,  0.7973]],\n",
      "\n",
      "        [[ 0.0650,  0.4974, -1.0731,  0.5541, -0.1693,  0.5412],\n",
      "         [-0.1594,  0.1868, -1.1472,  0.3560, -0.3313,  0.7696],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3281, -0.1330, -0.7934,  0.0572, -0.3417,  0.7438],\n",
      "         [-0.2204, -0.0504, -0.3107,  0.0531, -0.1435,  0.3357],\n",
      "         [-0.2804, -0.1221, -0.6816,  0.0222, -0.2906,  0.6775]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0650,  0.4974, -1.0731,  0.5541, -0.1693,  0.5412],\n",
       "         [-0.1948, -0.0841, -0.5629,  0.0543, -0.2391,  0.4749],\n",
       "         [-0.1097,  0.1279, -0.7878,  0.2442, -0.2276,  0.5287],\n",
       "         [-0.3105,  0.0014, -1.0834,  0.2069, -0.3874,  0.8901],\n",
       "         [-0.3780, -0.0176, -1.0250,  0.2195, -0.3868,  0.8606],\n",
       "         [-0.3840, -0.1352, -0.8097,  0.0670, -0.3519,  0.7973]],\n",
       "\n",
       "        [[ 0.0650,  0.4974, -1.0731,  0.5541, -0.1693,  0.5412],\n",
       "         [-0.1594,  0.1868, -1.1472,  0.3560, -0.3313,  0.7696],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3281, -0.1330, -0.7934,  0.0572, -0.3417,  0.7438],\n",
       "         [-0.2204, -0.0504, -0.3107,  0.0531, -0.1435,  0.3357],\n",
       "         [-0.2804, -0.1221, -0.6816,  0.0222, -0.2906,  0.6775]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,in_dim,out_dim,context_length,drop_rate):\n",
    "        super().__init__()\n",
    "        self.kw=torch.nn.Linear(in_dim,out_dim)\n",
    "        print(self.kw)\n",
    "        self.qw=torch.nn.Linear(in_dim,out_dim)\n",
    "        self.vw=torch.nn.Linear(in_dim,out_dim)\n",
    "        self.dropout=torch.nn.Dropout(drop_rate)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,t,in_dim=x.shape\n",
    "\n",
    "        key=self.kw(x)\n",
    "        print(key)\n",
    "        query=self.qw(x)\n",
    "        value=self.vw(x)\n",
    "\n",
    "        attn_score=query@key.transpose(1,2)\n",
    "\n",
    "        attn_score.masked_fill_(self.mask.bool()[:t,:t],-torch.inf)\n",
    "        print(attn_score)\n",
    "        attn_weight=torch.softmax(attn_score/key.shape[-1]**0.5,dim=-1)\n",
    "        print(attn_weight)\n",
    "        \n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "        \n",
    "        context_vectore=attn_weight@value\n",
    "\n",
    "\n",
    "        print(context_vectore)\n",
    "\n",
    "        return context_vectore\n",
    "\n",
    "\n",
    "\n",
    "x=CausalSelfAttention(batch.shape[-1],6,batch.shape[1],0.3)\n",
    "x(batch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=6, bias=True)\n",
      "Linear(in_features=3, out_features=6, bias=True)\n",
      "Linear(in_features=3, out_features=6, bias=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([head(x) \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m x(batch)\n",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, in_dim, out_dim, context_length, drop_rate, num_head)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,in_dim,out_dim,context_length,drop_rate,num_head):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList([CausalSelfAttention(in_dim,out_dim,context_length,drop_rate) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_head)])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1977\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1977\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1978\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1979\u001b[0m         )\n\u001b[1;32m   1980\u001b[0m     remove_from(\n\u001b[1;32m   1981\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m,\n\u001b[1;32m   1982\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters,\n\u001b[1;32m   1983\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers,\n\u001b[1;32m   1984\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set,\n\u001b[1;32m   1985\u001b[0m     )\n\u001b[1;32m   1986\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,context_length,drop_rate,num_head):\n",
    "        self.head=torch.nn.ModuleList([CausalSelfAttention(in_dim,out_dim,context_length,drop_rate) for _ in range(num_head)])\n",
    "        print(self.head)\n",
    "    def forward(self,x):\n",
    "        print(self.head)\n",
    "        return torch.cat([head(x) for head in self.head],dim=-1)\n",
    "    \n",
    "x=MultiHeadAttention(batch.shape[-1],6,batch.shape[1],0.3,3)\n",
    "x(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=torch.nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb=torch.nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb=torch.nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks=torch.nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "\n",
    "        self.final_norm=LayerNorm(cfg['emb_dim'])#implemented\n",
    "        self.output_head=torch.nn.Linear(cfg['emb_dim'], cfg['vocab_size'])\n",
    "\n",
    "    \n",
    "    def forward(self,in_idx):#input ID\n",
    "        batch_size,seq_len=in_idx.shape()\n",
    "        tok_emb=self.tok_emb(in_idx)\n",
    "        pos_emb=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_emb+pos_emb\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)#transformerblock implemented\n",
    "        logits=self.output_head(x)\n",
    "\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
