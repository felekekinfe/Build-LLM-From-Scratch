{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    'vocab_size':50257,\n",
    "    'context_length':1024,\n",
    "    'emb_dim':768,\n",
    "    'n_head':12,\n",
    "    'n_layers':12,\n",
    "    'drop_rate':0.1,\n",
    "    'qkv_bias':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "\n",
    "        return self.scale*norm_x+self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,emb_dim,out_dim,context_length,drop_rate):\n",
    "        super().__init__()\n",
    "        self.kw=torch.nn.Linear(emb_dim,out_dim)\n",
    "        \n",
    "        self.qw=torch.nn.Linear(emb_dim,out_dim)\n",
    "        self.vw=torch.nn.Linear(emb_dim,out_dim)\n",
    "        self.dropout=torch.nn.Dropout(drop_rate)\n",
    "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,t,emb_dim=x.shape\n",
    "\n",
    "        key=self.kw(x)\n",
    "        query=self.qw(x)\n",
    "        value=self.vw(x)\n",
    "\n",
    "        attn_score=query@key.transpose(1,2)\n",
    "        attn_score.masked_fill_(self.mask.bool()[:t,:t],-torch.inf)\n",
    "        \n",
    "        attn_weight=torch.softmax(attn_score/key.shape[-1]**0.5,dim=-1)\n",
    "        attn_weight=self.dropout(attn_weight)\n",
    "        \n",
    "        context_vectore=attn_weight@value\n",
    "        \n",
    "        return context_vectore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key tensor([[[ 0.0919,  0.5472, -0.1305],\n",
      "         [ 0.1653,  0.5250,  0.0755],\n",
      "         [ 0.1538,  0.5255,  0.0464],\n",
      "         [ 0.0212,  0.3707, -0.0217],\n",
      "         [-0.1513,  0.4570, -0.5829],\n",
      "         [ 0.1455,  0.3767,  0.2710]],\n",
      "\n",
      "        [[ 0.0919,  0.5472, -0.1305],\n",
      "         [ 0.1653,  0.5250,  0.0755],\n",
      "         [ 0.1538,  0.5255,  0.0464],\n",
      "         [ 0.0212,  0.3707, -0.0217],\n",
      "         [-0.1513,  0.4570, -0.5829],\n",
      "         [ 0.1455,  0.3767,  0.2710]]], grad_fn=<ViewBackward0>)\n",
      "key tensor([[[-0.5557,  0.0300, -0.1206],\n",
      "         [-0.9426, -0.4177,  0.0160],\n",
      "         [-0.9397, -0.4113, -0.0018],\n",
      "         [-0.6286, -0.4219, -0.1434],\n",
      "         [-0.6830, -0.2513, -0.4390],\n",
      "         [-0.6829, -0.4860,  0.0425]],\n",
      "\n",
      "        [[-0.5557,  0.0300, -0.1206],\n",
      "         [-0.9426, -0.4177,  0.0160],\n",
      "         [-0.9397, -0.4113, -0.0018],\n",
      "         [-0.6286, -0.4219, -0.1434],\n",
      "         [-0.6830, -0.2513, -0.4390],\n",
      "         [-0.6829, -0.4860,  0.0425]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1967e+00, -5.0772e-01,  1.0888e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [ 8.2270e-01, -3.0353e-01,  5.5394e-01,  2.0959e-01, -8.1366e-01,\n",
       "          -4.8994e-02],\n",
       "         [ 1.1090e+00, -4.0311e-01,  7.4355e-01,  2.6713e-01, -1.0503e+00,\n",
       "          -6.8118e-02],\n",
       "         [ 1.0121e+00, -4.1737e-01,  7.4802e-01,  2.1148e-01, -9.9142e-01,\n",
       "          -1.3058e-01],\n",
       "         [ 1.1730e+00, -3.7479e-01,  8.0469e-01,  7.6060e-02, -3.0298e-01,\n",
       "          -2.1087e-02],\n",
       "         [ 7.0502e-01, -2.1542e-01,  4.3351e-01, -6.9874e-03, -2.6695e-01,\n",
       "          -8.3627e-02]],\n",
       "\n",
       "        [[ 1.1967e+00, -5.0772e-01,  1.0888e+00,  7.8556e-02, -9.8259e-01,\n",
       "          -3.4292e-01],\n",
       "         [ 1.4401e+00, -5.6548e-01,  1.1157e+00,  2.4585e-01, -1.2671e+00,\n",
       "          -2.0726e-01],\n",
       "         [ 1.5236e+00, -5.7905e-01,  1.1209e+00,  1.5419e-01, -8.1630e-01,\n",
       "          -1.3966e-01],\n",
       "         [ 1.1485e+00, -4.3633e-01,  8.4431e-01,  1.8317e-01, -9.5842e-01,\n",
       "           9.7165e-03],\n",
       "         [ 1.3956e+00, -4.7793e-01,  9.4919e-01,  1.2340e-01, -9.2286e-01,\n",
       "          -2.7591e-02],\n",
       "         [ 1.3715e+00, -5.1451e-01,  9.4087e-01,  1.3531e-01, -8.0938e-01,\n",
       "          -1.2682e-03]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self,emb_dim,out_dim,context_length,drop_rate,n_head):\n",
    "        super().__init__()\n",
    "        assert (out_dim%n_head==0)\n",
    "        head_dim=out_dim//n_head\n",
    "        self.head=torch.nn.ModuleList([CausalSelfAttention(emb_dim,head_dim,context_length,drop_rate) for _ in range(n_head)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.head],dim=-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(torch.nn.Module):\n",
    "\n",
    "    def forward(self,x):\n",
    "        sqrt_2_over_pi = math.sqrt(2.0 / math.pi)\n",
    "        res=0.5 * x * (1 + torch.tanh(sqrt_2_over_pi * (x + 0.044715 * x**3)))\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.layer=torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, 4*emb_dim),\n",
    "            GELU(),\n",
    "            torch.nn.Linear(4*emb_dim,emb_dim)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config=GPT_CONFIG_124M\n",
    "        self.layer_norm=LayerNorm(config['emb_dim'])\n",
    "        self.mha=MultiHeadAttention(config['emb_dim'],config['emb_dim'],config['context_length'],config['drop_rate'],config['n_head'])\n",
    "        self.dropout=torch.nn.Dropout(config['drop_rate'])\n",
    "        self.ff=FeedForward(config['emb_dim'])\n",
    "\n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        l1_x=self.layer_norm(x)\n",
    "        mha=self.mha(l1_x)\n",
    "        dropout=self.dropout(mha)\n",
    "\n",
    "        x=shortcut+dropout\n",
    "        l2_x=self.layer_norm(x)\n",
    "        ff=self.ff(l2_x)\n",
    "        dropout=self.dropout(ff)\n",
    "\n",
    "        out=dropout+l2_x\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=torch.nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb=torch.nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.drop_emb=torch.nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks=torch.nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "\n",
    "        self.final_norm=LayerNorm(cfg['emb_dim'])#implemented\n",
    "        self.output_head=torch.nn.Linear(cfg['emb_dim'], cfg['vocab_size'])\n",
    "\n",
    "    \n",
    "    def forward(self,in_idx):#input ID\n",
    "        batch_size,seq_len=in_idx.shape()\n",
    "        tok_emb=self.tok_emb(in_idx)\n",
    "        pos_emb=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_emb+pos_emb\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)#transformerblock implemented\n",
    "        logits=self.output_head(x)\n",
    "\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
