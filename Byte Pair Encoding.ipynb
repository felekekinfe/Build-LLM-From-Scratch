{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize byte pair encoding\n",
    "\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 466, 345, 588, 6891, 30, 50256, 3763, 1312, 588]\n"
     ]
    }
   ],
   "source": [
    "#text to be encoded\n",
    "text=(\n",
    "    'hello do you like coffee?<|endoftext|> yes i like'\n",
    ")\n",
    "#call encode method which return ids of subword token\n",
    "integers=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello do you like coffee?<|endoftext|> yes i like\n"
     ]
    }
   ],
   "source": [
    "#now convert back token id back to text or decode\n",
    "\n",
    "strings=tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets implement BPE from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}#maps ID to string of character\n",
    "str_to_id={}#maps string to id inverse of vocab\n",
    "merges={}# maps (id1,id2) to new id eg id12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man sat on the chair\n"
     ]
    }
   ],
   "source": [
    "text='the man sat on the chair'\n",
    "vocab_size=50\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_man_sat_on_the_chair\n"
     ]
    }
   ],
   "source": [
    "text=text.replace(' ','_')# replace space with _ eg hey you becomes hey_you\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', 'a', 'c', 'e', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(set(text)) #sorted unique characters\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: 'a', 2: 'c', 3: 'e', 4: 'h', 5: 'i', 6: 'm', 7: 'n', 8: 'o', 9: 'r', 10: 's', 11: 't'}\n"
     ]
    }
   ],
   "source": [
    "vocab={i:c for i,c in enumerate(chars)}#initialize vocabulary\n",
    "str_to_id={c:i for i,c in enumerate(chars)}\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 4, 3, 0, 6, 1, 7, 0, 10, 1, 11, 0, 8, 7, 0, 11, 4, 3, 0, 2, 4, 1, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "token_ids=[str_to_id[i] for i in text] #token id in the order of how the characters comes in the text not sorted\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find most frequent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4)\n",
      "19\n",
      "24\n",
      "[11, 4, 3, 0, 6, 1, 7, 0, 10, 1, 11, 0, 8, 7, 0, 11, 4, 3, 0, 2, 4, 1, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_frequent_pairs(token_ids):\n",
    "    pairs=Counter(zip(token_ids,token_ids[1:])) #the zip function help us to do sliding window with size two read about zip()\n",
    "    print(max(pairs))\n",
    "    print(len(pairs))\n",
    "\n",
    "    return max(pairs,key=pairs.get) if pairs else None\n",
    "find_most_frequent_pairs(token_ids)\n",
    "\n",
    "print(len(token_ids))\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace all occurence of pair with new id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 3, 0, 6, 1, 7, 0, 10, 1, 11, 0, 8, 7, 0, 77, 3, 0, 2, 4, 1, 5, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_pair(token_ids,pair,new_id):\n",
    "    res=[121]\n",
    "    i=0\n",
    "    while i<len(token_ids):\n",
    "        #check that atleast two id exists\n",
    "        if i < len(token_ids)-1 and (token_ids[i],token_ids[i+1])==pair:\n",
    "            res.append(new_id)\n",
    "            i+=2\n",
    "        else:\n",
    "            res.append(token_ids[i])\n",
    "            i+=1\n",
    "    return res\n",
    "            \n",
    "\n",
    "replace_pair(token_ids,(11, 4),77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(11, 4): 2, (4, 3): 2, (3, 0): 2, (7, 0): 2, (0, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 11): 1, (0, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(12, 3): 2, (3, 0): 2, (7, 0): 2, (0, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 12): 1, (0, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(13, 0): 2, (7, 0): 2, (0, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 13): 1, (0, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(7, 0): 2, (14, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(14, 6): 1, (6, 1): 1, (1, 15): 1, (15, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(16, 1): 1, (1, 15): 1, (15, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(17, 15): 1, (15, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(18, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(19, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(20, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(21, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(22, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(23, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(24, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(25, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(26, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(27, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(28, 5): 1, (5, 9): 1})\n",
      "Counter({(29, 9): 1})\n",
      "Counter()\n",
      "[30]\n"
     ]
    }
   ],
   "source": [
    "def merge_frequent(token_ids):\n",
    "    new_id=len(vocab)\n",
    "    #merge until vocabulary size is reached\n",
    "\n",
    " #vocab size must be greater than len(vocab) initial without it doesnt make sense\n",
    "    while new_id<vocab_size:\n",
    "        pair=find_most_frequent_pairs(token_ids)\n",
    "        if not pair:\n",
    "            break\n",
    "        merges[pair]=new_id\n",
    "        token_ids=replace_pair(token_ids,pair,new_id)\n",
    "        \n",
    "        merged_str=vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "        vocab[new_id]=merged_str\n",
    "        \n",
    "\n",
    "        str_to_id[merged_str]=new_id\n",
    "\n",
    "        new_id+=1\n",
    "\n",
    "    print(token_ids)\n",
    "\n",
    "\n",
    "\n",
    "merge_frequent(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(11, 4): 12, (12, 3): 13, (13, 0): 14, (7, 0): 15, (14, 6): 16, (16, 1): 17, (17, 15): 18, (18, 10): 19, (19, 1): 20, (20, 11): 21, (21, 0): 22, (22, 8): 23, (23, 15): 24, (24, 14): 25, (25, 2): 26, (26, 4): 27, (27, 1): 28, (28, 5): 29, (29, 9): 30}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\n",
      "{0: '_', 1: 'a', 2: 'c', 3: 'e', 4: 'h', 5: 'i', 6: 'm', 7: 'n', 8: 'o', 9: 'r', 10: 's', 11: 't', 12: 'th', 13: 'the', 14: 'the_', 15: 'n_', 16: 'the_m', 17: 'the_ma', 18: 'the_man_', 19: 'the_man_s', 20: 'the_man_sa', 21: 'the_man_sat', 22: 'the_man_sat_', 23: 'the_man_sat_o', 24: 'the_man_sat_on_', 25: 'the_man_sat_on_the_', 26: 'the_man_sat_on_the_c', 27: 'the_man_sat_on_the_ch', 28: 'the_man_sat_on_the_cha', 29: 'the_man_sat_on_the_chai', 30: 'the_man_sat_on_the_chair'}\n"
     ]
    }
   ],
   "source": [
    "def encode(text):# str to id aka integer\n",
    "\n",
    "    text=text.replace(' ','_')\n",
    "\n",
    "    #convert to token id from already trained or created\n",
    "\n",
    "    token_ids=[str_to_id[i] for i in text]\n",
    "    #apply merge\n",
    "    while len(token_ids)>1:\n",
    "        #find the earliest merge,lowest id\n",
    "        current_pair=None\n",
    "        current_id=float('inf')\n",
    "        for pair in zip(token_ids,token_ids[1:]):\n",
    "            if pair in merges and merges[pair]<current_id:\n",
    "                current_pair=pair\n",
    "                current_id=merges[pair]\n",
    "        if current_pair is None:#if current pair is not in the merge break the loop\n",
    "            break\n",
    "        token_ids=replace_pair(token_ids,current_pair,current_id)\n",
    "    print(token_ids)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "encode(text)\n",
    "\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         text\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mtoken\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(token_ids)\u001b[0m\n\u001b[1;32m      3\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m token_ids:\n\u001b[0;32m----> 5\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     text\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mtoken\n",
      "\u001b[0;31mKeyError\u001b[0m: 13"
     ]
    }
   ],
   "source": [
    "def decode(token_ids):\n",
    "\n",
    "    text=''\n",
    "    for id in token_ids:\n",
    "        token=vocab[id]\n",
    "        token=token.replace('_',' ')\n",
    "        text+=token\n",
    "    return text\n",
    "decode([13, 5, 4, 18, 1, 7, 2, 8, 18, 1, 12, 2, 13, 18, 1, 9, 8, 18, 1, 13, 5, 4, 18, 1, 3, 5, 2, 6, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE From Scratch With OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BPE_From_Scratch:\n",
    "    def __init__(self):\n",
    "        self.vocabulary={}\n",
    "        self.token_to_id={}\n",
    "        self.merges={}\n",
    "\n",
    "\n",
    "    def frequent(self,token_ids):\n",
    "        pair=Counter(zip(token_ids,token_ids[1:]))\n",
    "\n",
    "        if pair:\n",
    "            return max(pair,key=pair.get)\n",
    "        return None\n",
    "\n",
    "        \n",
    "\n",
    "    def replace(self,token_ids,pair,new_id):\n",
    "        res=[]\n",
    "        i=0\n",
    "\n",
    "        while i<len(token_ids):\n",
    "            \n",
    "            if i<len(token_ids)-1 and (token_ids[i],token_ids[i+1])==pair:\n",
    "                res.append(new_id)\n",
    "                i+=2\n",
    "            else:\n",
    "                res.append(token_ids[i])\n",
    "                i+=1\n",
    "        return res\n",
    "\n",
    "    def __merge(self,token_ids,vocab_size):#private method\n",
    "\n",
    "        new_id=len(self.vocabulary)\n",
    "\n",
    "        while new_id<vocab_size:\n",
    "            pair=self.frequent(token_ids)\n",
    "            \n",
    "\n",
    "            if pair is None:\n",
    "                break\n",
    "\n",
    "            self.merges[pair]=new_id\n",
    "\n",
    "            token_ids=self.replace(token_ids,pair,new_id)\n",
    "\n",
    "            merged_char=self.vocabulary[pair[0]]+self.vocabulary[pair[1]]\n",
    "\n",
    "            self.vocabulary[new_id]=merged_char\n",
    "            self.token_to_id[merged_char]=new_id\n",
    "\n",
    "            new_id+=1\n",
    "        return self.vocabulary\n",
    "        \n",
    "    \n",
    "    def train(self, text, vocab_size):\n",
    "        text = text.replace(' ', '<space>')\n",
    "        chars = sorted(set(text))\n",
    "        \n",
    "        # add <unk> as a special token\n",
    "        chars.append('<unk>')\n",
    "\n",
    "        self.vocabulary = {i: c for i, c in enumerate(chars)}\n",
    "        self.token_to_id = {c: i for i, c in enumerate(chars)}\n",
    "\n",
    "        token_ids = [self.token_to_id.get(c, self.token_to_id['<unk>']) for c in text]\n",
    "\n",
    "        generated_vocab = self.__merge(token_ids, vocab_size)\n",
    "\n",
    "        return generated_vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def encode(self, text):\n",
    "        print('merges', self.merges)\n",
    "        text = text.replace(' ', '<space>')\n",
    "        \n",
    "        # replace unseen characters with <unk> id\n",
    "        token_ids = [self.token_to_id.get(c, self.token_to_id['<unk>']) for c in text]\n",
    "\n",
    "        while True:\n",
    "            pair = self.frequent(token_ids)\n",
    "            if pair not in self.merges:\n",
    "                break  # no known pair in merges\n",
    "\n",
    "            new_id = self.merges[pair]\n",
    "            print('before', token_ids)\n",
    "            print('pair', pair, '----->ID', new_id)\n",
    "            token_ids = self.replace(token_ids, pair, new_id)\n",
    "            print('after', token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def decode(self,token_ids):\n",
    "        text=''\n",
    "        for id in token_ids:\n",
    "            char=self.vocabulary.get(id,' unk_id ')\n",
    "            text+=char\n",
    "        text=text.replace('<space>',' ')\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges {(0, 12): 15, (15, 10): 16, (16, 2): 17, (17, 3): 18, (18, 4): 19, (19, 1): 20, (13, 5): 21, (21, 4): 22, (22, 20): 23, (8, 20): 24, (23, 7): 25, (25, 2): 26, (26, 24): 27, (27, 12): 28, (28, 2): 29, (29, 13): 30, (30, 20): 31, (31, 9): 32, (32, 24): 33, (33, 23): 34, (34, 3): 35, (35, 5): 36, (36, 2): 37, (37, 6): 38, (38, 11): 39}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the man sa'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe=BPE_From_Scratch()\n",
    "bpe.train(text=text,vocab_size=100)\n",
    "\n",
    "bpe.encode(\"The enco\")\n",
    "bpe.decode([29])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
