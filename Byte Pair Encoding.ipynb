{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize byte pair encoding\n",
    "\n",
    "tokenizer=tiktoken.get_encoding('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 466, 345, 588, 6891, 30, 50256, 3763, 1312, 588]\n"
     ]
    }
   ],
   "source": [
    "#text to be encoded\n",
    "text=(\n",
    "    'hello do you like coffee?<|endoftext|> yes i like'\n",
    ")\n",
    "#call encode method which return ids of subword token\n",
    "integers=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello do you like coffee?<|endoftext|> yes i like\n"
     ]
    }
   ],
   "source": [
    "#now convert back token id back to text or decode\n",
    "\n",
    "strings=tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets implement BPE from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}#maps ID to string oe character\n",
    "str_to_id={}#maps string to id inverse of vocab\n",
    "merges={}# maps (id1,id2) to new id eg id12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man sat on the chair\n"
     ]
    }
   ],
   "source": [
    "text='the man sat on the chair'\n",
    "vocab_size=50\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_man_sat_on_the_chair\n"
     ]
    }
   ],
   "source": [
    "text=text.replace(' ','_')# replace space with _ eg hey you becomes hey_you\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', 'a', 'c', 'e', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(set(text)) #sorted unique characters\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: 'a', 2: 'c', 3: 'e', 4: 'h', 5: 'i', 6: 'm', 7: 'n', 8: 'o', 9: 'r', 10: 's', 11: 't'}\n"
     ]
    }
   ],
   "source": [
    "vocab={i:c for i,c in enumerate(chars)}#initialize vocabulary\n",
    "str_to_id={c:i for i,c in enumerate(chars)}\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 4, 3, 0, 6, 1, 7, 0, 10, 1, 11, 0, 8, 7, 0, 11, 4, 3, 0, 2, 4, 1, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "token_ids=[str_to_id[i] for i in text] #token id in the order of how the characters comes in the text not sorted\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find most frequent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4)\n",
      "19\n",
      "24\n",
      "[11, 4, 3, 0, 6, 1, 7, 0, 10, 1, 11, 0, 8, 7, 0, 11, 4, 3, 0, 2, 4, 1, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_frequent_pairs(token_ids):\n",
    "    pairs=Counter(zip(token_ids,token_ids[1:])) #the zip function help us to do sliding window with size two read about zip()\n",
    "    print(max(pairs))\n",
    "    print(len(pairs))\n",
    "\n",
    "    return max(pairs,key=pairs.get) if pairs else None\n",
    "find_most_frequent_pairs(token_ids)\n",
    "\n",
    "print(len(token_ids))\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace all occurence of pair with new id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77, 3, 0, 6, 1, 7, 0, 10, 1, 11, 0, 8, 7, 0, 77, 3, 0, 2, 4, 1, 5, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_pair(token_ids,pair,new_id):\n",
    "    res=[121]\n",
    "    i=0\n",
    "    while i<len(token_ids):\n",
    "        #check that atleast two id exists\n",
    "        if i < len(token_ids)-1 and (token_ids[i],token_ids[i+1])==pair:\n",
    "            res.append(new_id)\n",
    "            i+=2\n",
    "        else:\n",
    "            res.append(token_ids[i])\n",
    "            i+=1\n",
    "    return res\n",
    "            \n",
    "\n",
    "replace_pair(token_ids,(11, 4),77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(11, 4): 2, (4, 3): 2, (3, 0): 2, (7, 0): 2, (0, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 11): 1, (0, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(12, 3): 2, (3, 0): 2, (7, 0): 2, (0, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 12): 1, (0, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(13, 0): 2, (7, 0): 2, (0, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 13): 1, (0, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(7, 0): 2, (14, 6): 1, (6, 1): 1, (1, 7): 1, (0, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 7): 1, (0, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(14, 6): 1, (6, 1): 1, (1, 15): 1, (15, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(16, 1): 1, (1, 15): 1, (15, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(17, 15): 1, (15, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(18, 10): 1, (10, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(19, 1): 1, (1, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(20, 11): 1, (11, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(21, 0): 1, (0, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(22, 8): 1, (8, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(23, 15): 1, (15, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(24, 14): 1, (14, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(25, 2): 1, (2, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(26, 4): 1, (4, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(27, 1): 1, (1, 5): 1, (5, 9): 1})\n",
      "Counter({(28, 5): 1, (5, 9): 1})\n",
      "Counter({(29, 9): 1})\n",
      "Counter()\n",
      "[30]\n"
     ]
    }
   ],
   "source": [
    "def merge_frequent(token_ids):\n",
    "    new_id=len(vocab)\n",
    "    #merge until vocabulary size is reached\n",
    "\n",
    " #vocab size must be greater than len(vocab) initial without it doesnt make sense\n",
    "    while new_id<vocab_size:\n",
    "        pair=find_most_frequent_pairs(token_ids)\n",
    "        if not pair:\n",
    "            break\n",
    "        merges[pair]=new_id\n",
    "        token_ids=replace_pair(token_ids,pair,new_id)\n",
    "        \n",
    "        merged_str=vocab[pair[0]] + vocab[pair[1]]\n",
    "\n",
    "        vocab[new_id]=merged_str\n",
    "        \n",
    "\n",
    "        str_to_id[merged_str]=new_id\n",
    "\n",
    "        new_id+=1\n",
    "\n",
    "    print(token_ids)\n",
    "\n",
    "\n",
    "\n",
    "merge_frequent(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(11, 4): 12, (12, 3): 13, (13, 0): 14, (7, 0): 15, (14, 6): 16, (16, 1): 17, (17, 15): 18, (18, 10): 19, (19, 1): 20, (20, 11): 21, (21, 0): 22, (22, 8): 23, (23, 15): 24, (24, 14): 25, (25, 2): 26, (26, 4): 27, (27, 1): 28, (28, 5): 29, (29, 9): 30}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\n",
      "{0: '_', 1: 'a', 2: 'c', 3: 'e', 4: 'h', 5: 'i', 6: 'm', 7: 'n', 8: 'o', 9: 'r', 10: 's', 11: 't', 12: 'th', 13: 'the', 14: 'the_', 15: 'n_', 16: 'the_m', 17: 'the_ma', 18: 'the_man_', 19: 'the_man_s', 20: 'the_man_sa', 21: 'the_man_sat', 22: 'the_man_sat_', 23: 'the_man_sat_o', 24: 'the_man_sat_on_', 25: 'the_man_sat_on_the_', 26: 'the_man_sat_on_the_c', 27: 'the_man_sat_on_the_ch', 28: 'the_man_sat_on_the_cha', 29: 'the_man_sat_on_the_chai', 30: 'the_man_sat_on_the_chair'}\n"
     ]
    }
   ],
   "source": [
    "def encode(text):# str to id aka integer\n",
    "\n",
    "    text=text.replace(' ','*')\n",
    "\n",
    "    #convert to token id from already trained or created\n",
    "\n",
    "    token_ids=[str_to_id[i] for i in text]\n",
    "    #apply merge\n",
    "    while len(token_ids)>1:\n",
    "        #find the earliest merge,lowest id\n",
    "        current_pair=None\n",
    "        current_id=float('inf')\n",
    "        for pair in zip(token_ids,token_ids[1:]):\n",
    "            if pair in merges and merges[pair]<current_id:\n",
    "                current_pair=pair\n",
    "                current_id=merges[pair]\n",
    "        if current_pair is None:#if current pair is not in the merge break the loop\n",
    "            break\n",
    "        token_ids=replace_pair(token_ids,current_pair,current_id)\n",
    "    print(token_ids)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "encode(text)\n",
    "\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the man sat on i'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(token_ids):\n",
    "\n",
    "    text=''\n",
    "    for id in token_ids:\n",
    "        token=vocab[id]\n",
    "        token=token.replace('_',' ')\n",
    "        text+=token\n",
    "    return text\n",
    "decode([19, 1, 11, 0, 8, 15, 5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE From Scratch With OOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE_From_Scratch:\n",
    "    def __init__(self):\n",
    "        self.vocabulary={}\n",
    "        self.token_to_id={}\n",
    "        self.merges={}\n",
    "\n",
    "\n",
    "    def frequent(self,token_ids):\n",
    "        pair=Counter(zip(token_ids,token_ids[1:]))\n",
    "\n",
    "        if pair:\n",
    "            return max(pair)\n",
    "        return None\n",
    "\n",
    "        \n",
    "\n",
    "    def replace(self,token_ids,pair,new_id):\n",
    "        res=[]\n",
    "        i=0\n",
    "\n",
    "        while i<len(token_ids):\n",
    "            if i<len(token_ids)-1 and (token_ids[i],token_ids[i+1])==pair:\n",
    "                res.append(new_id)\n",
    "                i+=2\n",
    "            else:\n",
    "                res.append(token_ids[i])\n",
    "                i+=1\n",
    "        return res\n",
    "\n",
    "    def merge(self,token_ids,vocab_size):\n",
    "\n",
    "        new_id=len(self.vocabulary)\n",
    "\n",
    "        while new_id<vocab_size:\n",
    "            pair=self.frequent(token_ids)\n",
    "            \n",
    "\n",
    "            if pair is None:\n",
    "                break\n",
    "\n",
    "            self.merges[pair]=new_id\n",
    "\n",
    "            token_ids=self.replace(token_ids,pair,new_id)\n",
    "\n",
    "            merged_char=self.vocabulary[pair[0]]+self.vocabulary[pair[1]]\n",
    "\n",
    "            self.vocabulary[new_id]=merged_char\n",
    "            self.token_to_id[merged_char]=new_id\n",
    "\n",
    "            new_id+=1\n",
    "        return self.vocabulary\n",
    "        \n",
    "    \n",
    "    def train(self,text,vocab_size):\n",
    "        text=text.replace(' ','<space>')\n",
    "        chars=sorted(set(text))\n",
    "        self.vocabulary={i:c for i,c in enumerate(chars)}\n",
    "        self.token_to_id={c:i for i,c in enumerate(chars)}\n",
    "\n",
    "        token_ids=[self.token_to_id[i] for i in text]\n",
    "\n",
    "        generated_vocab=self.merge(token_ids,vocab_size)\n",
    "\n",
    "        return generated_vocab\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def encode(self,text):\n",
    "        pass\n",
    "    def decode(self,token_ids):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '_',\n",
       " 1: 'a',\n",
       " 2: 'c',\n",
       " 3: 'e',\n",
       " 4: 'h',\n",
       " 5: 'i',\n",
       " 6: 'm',\n",
       " 7: 'n',\n",
       " 8: 'o',\n",
       " 9: 'r',\n",
       " 10: 's',\n",
       " 11: 't',\n",
       " 12: 'th',\n",
       " 13: 'the',\n",
       " 14: 'the_',\n",
       " 15: 'the_m',\n",
       " 16: 'the_ma',\n",
       " 17: 'the_man',\n",
       " 18: 'the_man_',\n",
       " 19: 'the_man_s'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe=BPE_From_Scratch()\n",
    "bpe.train(text=text,vocab_size=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
