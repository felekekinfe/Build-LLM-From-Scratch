{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 4358\n"
     ]
    }
   ],
   "source": [
    "#read the txt file\n",
    "with open('data/astu overview.txt','r',encoding='utf-8') as f:\n",
    "    raw_text=f.read()\n",
    "print(f'number of characters:',len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h e l l o ,   e t h i o p i a ? .   t h i s ,   i s   n o t h i n g\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text='hello, ethiopia?. this, is nothing'\n",
    "token=re.sub(r'([,.?]|\\s)',r'\\1',text)\n",
    "token=' '.join(token)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'ethiopia', '?', '.', 'this', ',', 'is', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "#remove white space\n",
    "#removing white space reduce memory and computing requirment\n",
    "#however some data is sensetive to their structure example python code indentaation and spacing in this case keep the white space\n",
    "\n",
    "result=[i for i in token if i.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['University', 'Name:', 'Adama', 'Science', 'and', 'Technology', 'University', '(ASTU)', 'or', 'also', 'called', 'Astu', 'Alternative', 'Names:ASTU', 'Location:', 'City:', 'Adama', '/', 'Nazret', 'Region:', 'Oromia', 'Country:', 'Ethiopia', 'Approx.', 'Coordinates:', '8.53째N,', '39.28째E', 'Establishment', 'History:', '1993:', 'Founded', 'as', 'Nazareth', 'Technical', 'College', '(NTC).', '2003:', 'Upgraded', 'to', 'Nazareth', 'College', 'of', 'Technical', 'Teacher', 'Education', '(NCTTE).', '2005:', 'Further', 'upgraded', 'to', 'Adama', 'University', '(AU).', '2011:', 'Designated', 'by', 'the', 'Ethiopian', 'Ministry', 'of', 'Education', 'as', 'a', 'Federal', 'University', 'of', 'Technology', 'and', 'renamed', 'Adama', 'Science', 'and', 'Technology', 'University', '(ASTU).', 'Type:', 'Public', 'University', 'Federal', 'University', 'of', 'Science', 'and', 'Technology', 'Mission', '(Core', 'Focus):', 'To', 'produce', 'highly', 'qualified', 'graduates', 'in', 'science,', 'engineering,', 'and', 'technology.', 'To', 'conduct']\n"
     ]
    }
   ],
   "source": [
    "#lets tokenize raw_text\n",
    "\n",
    "processed=re.split(r'(/|\\s)',raw_text)\n",
    "processed=[i for i in processed if i.strip()]\n",
    "print(processed[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n"
     ]
    }
   ],
   "source": [
    "#vocabulary is a list of token which are sorted and map with id sometime unique\n",
    "\n",
    "all_words=sorted(set(processed))\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "vocab={token:indx for indx,token in enumerate(all_words)}\n",
    "print(vocab['University'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={int:token for token,int in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        processed=sorted(set(re.split(r'(/|\\s)',text)))\n",
    "        processed=[token for token in processed if token.strip()]\n",
    "        id=[self.str_to_int[token] for token in processed]\n",
    "\n",
    "        return id\n",
    "    def decode(self,ids):\n",
    "        text=' '.join([self.int_to_str[i] for i in ids])\n",
    "        #replace space before the specified punctuations\n",
    "        token=re.sub(r'([,.?]|\\s)',r'\\1',text)\n",
    "        return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178]\n"
     ]
    }
   ],
   "source": [
    "tokenize=TokenizerV1(vocab)\n",
    "\n",
    "text='University'\n",
    "id=tokenize.encode(text)\n",
    "print(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feleke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenize\u001b[38;5;241m=\u001b[39mTokenizerV1(vocab)\n\u001b[1;32m      5\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeleke\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mid\u001b[39m)\n",
      "Cell \u001b[0;32mIn[70], line 8\u001b[0m, in \u001b[0;36mTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      6\u001b[0m processed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(/|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m,text)))\n\u001b[1;32m      7\u001b[0m processed\u001b[38;5;241m=\u001b[39m[token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m processed \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m processed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m\n",
      "Cell \u001b[0;32mIn[70], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m processed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(/|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m,text)))\n\u001b[1;32m      7\u001b[0m processed\u001b[38;5;241m=\u001b[39m[token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m processed \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m processed]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feleke'"
     ]
    }
   ],
   "source": [
    "#what if we give a word not present in the vocab\n",
    "#one reson llm should train on large dataset is to expand its abbility to represent huge vocab\n",
    "tokenize=TokenizerV1(vocab)\n",
    "\n",
    "text='feleke'\n",
    "id=tokenize.encode(text)\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding special context token \n",
    "# what if the word is not in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(ASTU)', '(ASTU).', '(AU).', '(BSc,', '(Core', '(Core', '(ICT)', '(MSc,', '(NCTTE).', '(NTC).', '(Note:', '(Schools', '(Science,', '(e.g.,', '(for', '(often', '(often', '/', '/', '/', '/', '/', '/', '1993:', '2003:', '2005:', '2011:', '39.28째E', '8.53째N,', 'Academic', 'Academic', 'Academic', 'Accommodation:', 'Adama', 'Adama', 'Adama', 'Adama', 'Adama', 'Adama', 'Address:', 'Administration,', 'Affairs,', 'Aligned', 'Alternative', 'Applied', 'Applied', 'Applied', 'Approx.', 'Architecture,', 'Areas', 'Astu', 'B.Tech)', 'Biology,', 'Biosciences', 'Biotechnology', 'Board.', 'Buildings:', 'Campus', 'Campus:', 'Centers:', 'Centers:', 'Central', 'Chemical', 'Chemistry,', 'City:', 'Civil', 'Collaborations:', 'Collaborative', 'College', 'College', 'Colleges', 'Communication', 'Computer', 'Computer', 'Computing', 'Construction', 'Contributes', 'Coordinates:', 'Country:', 'Designated', 'Dining', 'Dormitories.', 'Draws', 'Education', 'Education', 'Electrical', 'Energy', 'Engineering', 'Engineering', 'Engineering', 'Engineering', 'Engineering', 'Engineering', 'Engineering(CSE),', 'Engineering,', 'Engineering,', 'Engineering,', 'Engineering,Materials', 'Engineering,electronics', 'Engineering.', 'Enrollment:', 'Environmental', 'Establishment', 'Ethiopia', 'Ethiopia', \"Ethiopia's\", \"Ethiopia's\", \"Ethiopia's\", 'Ethiopia.', 'Ethiopia.', 'Ethiopian', 'Exact', 'Examples:', 'Examples:', 'Examples:', 'Facilities:', 'Facilities:', 'Federal', 'Federal', 'Focus', 'Focus):', 'Focus):', 'Focus:', 'For', 'Founded', 'Further', 'Geology,', 'Governance:', 'Governed', 'History:', 'Humanities', 'ICT', 'ICT,', 'Incubation', 'Industrial', 'Industry', 'Information', 'Infrastructure:', 'Institute', 'Institutes):', 'Internships', 'Irrigation', 'Key', 'Key', 'Laboratories:', 'Lecture', 'Led', 'Library:', 'Linkages', 'Located', 'Location:', 'M.Tech,', 'Main', 'Management', 'Manufacturing', 'Materials', 'Mathematics).', 'Mathematics,', 'May', 'Mechanical', 'Ministry', 'Mission', 'MoUs', 'Name:', 'Names:ASTU', 'Natural', 'Nazareth', 'Nazareth', 'Nazret', 'Nazret,', 'Nazret.', 'Offered:', 'One', 'Oromia', 'Partnerships', 'PhD)', 'Pharmacy.', 'Physical', 'Physics,', 'Plays', 'Postgraduate', 'Postgraduate', 'President,', 'Presidents', 'Program', 'Programs', 'Public', 'Recreation:', 'Region:', 'Renewable', 'Research', 'Research', 'Research', 'Resources', 'Resources', 'Role:', 'S&T', 'STEM', 'School', 'School', 'School', 'School', 'Science', 'Science', 'Science', 'Science', 'Science', 'Science', 'Sciences', 'Sciences', 'Significance', 'Social', 'Software', 'Specialized', 'Sports', 'Sports', 'Structure', 'Student', 'Student', 'Student', 'Studies', 'Teacher', 'Technical', 'Technical', 'Technologies', 'Technology', 'Technology', 'Technology', 'Technology', 'Technology', 'Technology', 'Technology', 'Technology', 'Technology', 'Technology,', 'The', 'To', 'To', 'To', 'To', 'To', 'To', 'To', 'Transfer,', 'Type:', 'Typically', 'Undergraduate', 'University', 'University', 'University', 'University', 'University', 'University', 'University', 'University', 'Upgraded', 'Vice', 'Vision', 'Water', 'Water', 'Website:', 'Workshops:', 'a', 'a', 'a', 'a', 'a', 'a', 'above', 'access.', 'across', 'advancement.', 'all', 'also', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'and', 'annually', 'applied', 'are', 'areas', 'areas', 'as', 'as', 'be', 'become', 'biology', 'by', 'by', 'by', 'cafeterias.', 'called', 'campus', 'can', 'center', 'centers', 'chemical,', 'classrooms.', 'communication', 'community', 'conduct', 'consultancy', 'contribute', 'crucial', 'dedicated', 'departmental', 'departments', 'departments)', 'development', 'development', 'development.', 'digital', 'direct', 'disciplines', 'disciplines.', 'education', 'efforts.', 'electrical,', 'energy,', 'engineering', 'engineering', 'engineering,', 'engineering.', 'enrolls', 'entrepreneurship.', 'etc.', 'etc.).', 'evolve.', 'excellence', 'externships', 'facilities.', 'faculty', 'fields', 'fields,', 'focusing', 'for', 'for', 'for', 'from', 'graduates', 'halls,', 'highly', 'host', 'human', 'impact.', 'in', 'in', 'in', 'in', 'in', 'include:', 'industrial', 'industrial', 'industrialization', 'industries.', 'industry.', 'innovation', 'innovation', 'institutes', 'institutions.', 'international', 'international', 'international', 'internet', 'key', 'labs', 'labs).', 'labs,', 'largest,', 'leading', 'libraries', 'library', 'like', 'local', 'materials', 'mechanical,', 'multiple', 'names', 'national', 'national', 'national', 'national', 'needs.', 'network,', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'of', 'often', 'on', 'on', 'or', 'or', 'other', 'partnerships', 'physical', 'physics,', 'postgraduate', 'practical', 'priorities', 'problem-solving', 'produce', 'programs.', 'projects', 'provide', 'public', 'qualified', 'recreational', 'regions', 'relevant', 'renamed', 'renewable', 'representative.)', 'research', 'research', 'research', 'research', 'research', 'research', 'research.', 'resources', 'resources.', 'role', 'roles)', 'schools', 'science', 'science', 'science', 'science,', 'science,', 'service', 'services.', 'significantly', 'skilled', 'societal', 'solutions', 'specialized', 'structure', 'student', 'students', 'students', 'students.', 'supplying', 'support', 'supportive', 'technological', 'technological', 'technology', 'technology.', 'technology.', 'technology.', 'the', 'the', 'their', 'thematic', 'thousands', 'to', 'to', 'to', 'to', 'to', 'to', 'training', 'transfer', 'transfer.', 'typical', 'undergraduate', 'universities', 'universities', 'university', 'upgraded', 'various', 'with', 'with', 'with', 'with', 'with', 'with', 'with', 'ww.astu.edu.et']\n",
      "536\n"
     ]
    }
   ],
   "source": [
    "all_tokens=sorted(processed)\n",
    "print(all_tokens)\n",
    "all_tokens.extend(['<|endoftext|>','<|unk|>'])\n",
    "vocab={token:int for int,token in enumerate(all_tokens)}\n",
    "print(vocab['<|unk|>'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={int:token for token,int in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        processed=sorted(set(re.split(r'(/|\\s)',text)))\n",
    "        processed=[token for token in processed if token.strip()]\n",
    "        processed=[ token if token in self.str_to_int\n",
    "                          else '<|unk|>' for token in processed]\n",
    "        id=[self.str_to_int[token] for token in processed]\n",
    "\n",
    "        return id\n",
    "    def decode(self,ids):\n",
    "        text=' '.join([self.int_to_str[i] for i in ids])\n",
    "        #replace space before the specified punctuations\n",
    "        token=re.sub(r'([,.?]|\\s)',r'\\1',text)\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> <|unk|>'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=TokenizerV2(vocab)\n",
    "text1='hey feleke'\n",
    "text2='hello'\n",
    "text='<|endoftext|>'.join((text1,text2))\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.encode(text1)\n",
    "id=tokenizer.encode(text1)\n",
    "tokenizer.decode(id)\n",
    "\n",
    "#gpt models only used <|endoftext|> only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
