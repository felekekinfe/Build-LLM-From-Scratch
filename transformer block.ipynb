{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    'vocab_size':50257,\n",
    "    'context_length':1024,\n",
    "    'emb_dim':768,\n",
    "    'n_head':12,\n",
    "    'n_layers':12,\n",
    "    'drop_rate':0.1,\n",
    "    'qkv_bias':False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "\n",
    "        return self.scale*norm_x+self.shift\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715 *torch.pow(x,3))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForWard(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers=torch.nn.Sequential(\n",
    "            torch.nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            torch.nn.Linear(4*cfg['emb_dim'],cfg['emb_dim']),\n",
    "            \n",
    "\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        return sel.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        self.norm1=LayerNorm(cfg['emb_dim'])\n",
    "        self.attn=MultiheadAttention(d_in=cfg['emb_dim'],\n",
    "                                     d_out=cfg['emb_dim'],\n",
    "                                     context_length=cfg['context_length'],\n",
    "                                     num_head=cfg['n_head'],\n",
    "                                     dropout=cfg['drop_rate'],\n",
    "                                     qkv_bias=cfg['qkv_bias'])#implementd in standalone .ipynb file\n",
    "\n",
    "        self.norm2=LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut=torch.dropout(cfg['drop_rate'])\n",
    "        self.ff=FeedForWard(cfg)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        #first shortcut\n",
    "        self.shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.attn(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+self.shortcut\n",
    "\n",
    "        #second shortcut\n",
    "\n",
    "        self.shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+self.shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
